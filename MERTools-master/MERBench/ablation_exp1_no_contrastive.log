====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v4', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=False, use_gated_fusion=True, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 44%|████▍     | 1487/3373 [00:00<00:00, 14860.09it/s] 88%|████████▊ | 2974/3373 [00:00<00:00, 11389.10it/s]100%|██████████| 3373/3373 [00:00<00:00, 11393.25it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 29%|██▊       | 968/3373 [00:00<00:00, 9658.50it/s] 57%|█████▋    | 1934/3373 [00:00<00:00, 8746.71it/s] 86%|████████▌ | 2894/3373 [00:00<00:00, 9069.35it/s]100%|██████████| 3373/3373 [00:00<00:00, 8878.37it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 52%|█████▏    | 1748/3373 [00:00<00:00, 17128.21it/s]100%|██████████| 3373/3373 [00:00<00:00, 15114.08it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 11212.24it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 13684.35it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 19401.90it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 15417.21it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 12108.68it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 16492.21it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 13879.39it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 11698.85it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14577.39it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1069; eval:0.2489; lr:0.000500
epoch:2; metric:emoval; train:0.2908; eval:0.3207; lr:0.000500
epoch:3; metric:emoval; train:0.4238; eval:0.4500; lr:0.000500
epoch:4; metric:emoval; train:0.5203; eval:0.4992; lr:0.000500
epoch:5; metric:emoval; train:0.5782; eval:0.5528; lr:0.000500
epoch:6; metric:emoval; train:0.6092; eval:0.4762; lr:0.000500
epoch:7; metric:emoval; train:0.6146; eval:0.5504; lr:0.000500
epoch:8; metric:emoval; train:0.6756; eval:0.5327; lr:0.000500
epoch:9; metric:emoval; train:0.6969; eval:0.5817; lr:0.000500
epoch:10; metric:emoval; train:0.7053; eval:0.5513; lr:0.000500
epoch:11; metric:emoval; train:0.7156; eval:0.5533; lr:0.000500
epoch:12; metric:emoval; train:0.7382; eval:0.4955; lr:0.000500
epoch:13; metric:emoval; train:0.7327; eval:0.5469; lr:0.000500
epoch:14; metric:emoval; train:0.7492; eval:0.5346; lr:0.000500
epoch:15; metric:emoval; train:0.7531; eval:0.5461; lr:0.000500
epoch:16; metric:emoval; train:0.7528; eval:0.5396; lr:0.000500
epoch:17; metric:emoval; train:0.7638; eval:0.5504; lr:0.000500
epoch:18; metric:emoval; train:0.7638; eval:0.5422; lr:0.000500
epoch:19; metric:emoval; train:0.7771; eval:0.5533; lr:0.000500
epoch:20; metric:emoval; train:0.7861; eval:0.5728; lr:0.000250
epoch:21; metric:emoval; train:0.8245; eval:0.5695; lr:0.000250
epoch:22; metric:emoval; train:0.8242; eval:0.5724; lr:0.000250
epoch:23; metric:emoval; train:0.8372; eval:0.5569; lr:0.000250
epoch:24; metric:emoval; train:0.8263; eval:0.6019; lr:0.000250
epoch:25; metric:emoval; train:0.8431; eval:0.5991; lr:0.000250
epoch:26; metric:emoval; train:0.8413; eval:0.5832; lr:0.000250
epoch:27; metric:emoval; train:0.8220; eval:0.5639; lr:0.000250
epoch:28; metric:emoval; train:0.8375; eval:0.5833; lr:0.000250
epoch:29; metric:emoval; train:0.8336; eval:0.5785; lr:0.000250
epoch:30; metric:emoval; train:0.8443; eval:0.5614; lr:0.000250
epoch:31; metric:emoval; train:0.8189; eval:0.5847; lr:0.000250
epoch:32; metric:emoval; train:0.8167; eval:0.6190; lr:0.000250
epoch:33; metric:emoval; train:0.7969; eval:0.5964; lr:0.000250
epoch:34; metric:emoval; train:0.7959; eval:0.5756; lr:0.000250
epoch:35; metric:emoval; train:0.8115; eval:0.6017; lr:0.000250
epoch:36; metric:emoval; train:0.7954; eval:0.5945; lr:0.000250
epoch:37; metric:emoval; train:0.7929; eval:0.5917; lr:0.000250
epoch:38; metric:emoval; train:0.7857; eval:0.5813; lr:0.000250
epoch:39; metric:emoval; train:0.8123; eval:0.5631; lr:0.000250
epoch:40; metric:emoval; train:0.7978; eval:0.5519; lr:0.000250
epoch:41; metric:emoval; train:0.7824; eval:0.5476; lr:0.000250
epoch:42; metric:emoval; train:0.7947; eval:0.5717; lr:0.000250
epoch:43; metric:emoval; train:0.7873; eval:0.5619; lr:0.000125
epoch:44; metric:emoval; train:0.8145; eval:0.5725; lr:0.000125
epoch:45; metric:emoval; train:0.8247; eval:0.6206; lr:0.000125
epoch:46; metric:emoval; train:0.8397; eval:0.5834; lr:0.000125
epoch:47; metric:emoval; train:0.8458; eval:0.6056; lr:0.000125
epoch:48; metric:emoval; train:0.8458; eval:0.6004; lr:0.000125
epoch:49; metric:emoval; train:0.8334; eval:0.5923; lr:0.000125
epoch:50; metric:emoval; train:0.8512; eval:0.5878; lr:0.000125
epoch:51; metric:emoval; train:0.8345; eval:0.5798; lr:0.000125
epoch:52; metric:emoval; train:0.8378; eval:0.5761; lr:0.000125
epoch:53; metric:emoval; train:0.8491; eval:0.5987; lr:0.000125
epoch:54; metric:emoval; train:0.8363; eval:0.5788; lr:0.000125
epoch:55; metric:emoval; train:0.8399; eval:0.5888; lr:0.000125
epoch:56; metric:emoval; train:0.8462; eval:0.5917; lr:0.000063
epoch:57; metric:emoval; train:0.8606; eval:0.6134; lr:0.000063
epoch:58; metric:emoval; train:0.8627; eval:0.5896; lr:0.000063
epoch:59; metric:emoval; train:0.8681; eval:0.5961; lr:0.000063
epoch:60; metric:emoval; train:0.8698; eval:0.6013; lr:0.000063
epoch:61; metric:emoval; train:0.8731; eval:0.5928; lr:0.000063
epoch:62; metric:emoval; train:0.8627; eval:0.5865; lr:0.000063
epoch:63; metric:emoval; train:0.8572; eval:0.5960; lr:0.000063
epoch:64; metric:emoval; train:0.8605; eval:0.5874; lr:0.000063
epoch:65; metric:emoval; train:0.8703; eval:0.5893; lr:0.000063
epoch:66; metric:emoval; train:0.8677; eval:0.5846; lr:0.000063
epoch:67; metric:emoval; train:0.8632; eval:0.6011; lr:0.000031
epoch:68; metric:emoval; train:0.8729; eval:0.6051; lr:0.000031
epoch:69; metric:emoval; train:0.8746; eval:0.6050; lr:0.000031
epoch:70; metric:emoval; train:0.8765; eval:0.5932; lr:0.000031
epoch:71; metric:emoval; train:0.8867; eval:0.5955; lr:0.000031
epoch:72; metric:emoval; train:0.8694; eval:0.5991; lr:0.000031
epoch:73; metric:emoval; train:0.8853; eval:0.6061; lr:0.000031
epoch:74; metric:emoval; train:0.8850; eval:0.6027; lr:0.000031
epoch:75; metric:emoval; train:0.8748; eval:0.6003; lr:0.000031
Early stopping at epoch 75, best epoch: 45
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 44, duration: 225.17925667762756 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0813; eval:0.3787; lr:0.000500
epoch:2; metric:emoval; train:0.3000; eval:0.5106; lr:0.000500
epoch:3; metric:emoval; train:0.4107; eval:0.5648; lr:0.000500
epoch:4; metric:emoval; train:0.5148; eval:0.4991; lr:0.000500
epoch:5; metric:emoval; train:0.5574; eval:0.4666; lr:0.000500
epoch:6; metric:emoval; train:0.5900; eval:0.5783; lr:0.000500
epoch:7; metric:emoval; train:0.6245; eval:0.5760; lr:0.000500
epoch:8; metric:emoval; train:0.6385; eval:0.5615; lr:0.000500
epoch:9; metric:emoval; train:0.6971; eval:0.5865; lr:0.000500
epoch:10; metric:emoval; train:0.6787; eval:0.6080; lr:0.000500
epoch:11; metric:emoval; train:0.7077; eval:0.6030; lr:0.000500
epoch:12; metric:emoval; train:0.7268; eval:0.6094; lr:0.000500
epoch:13; metric:emoval; train:0.7274; eval:0.5769; lr:0.000500
epoch:14; metric:emoval; train:0.7317; eval:0.5936; lr:0.000500
epoch:15; metric:emoval; train:0.7343; eval:0.6083; lr:0.000500
epoch:16; metric:emoval; train:0.7489; eval:0.6087; lr:0.000500
epoch:17; metric:emoval; train:0.7655; eval:0.5765; lr:0.000500
epoch:18; metric:emoval; train:0.7321; eval:0.5883; lr:0.000500
epoch:19; metric:emoval; train:0.7709; eval:0.5491; lr:0.000500
epoch:20; metric:emoval; train:0.7710; eval:0.5901; lr:0.000500
epoch:21; metric:emoval; train:0.7901; eval:0.6051; lr:0.000500
epoch:22; metric:emoval; train:0.7870; eval:0.6058; lr:0.000500
epoch:23; metric:emoval; train:0.7905; eval:0.5951; lr:0.000250
epoch:24; metric:emoval; train:0.8192; eval:0.6161; lr:0.000250
epoch:25; metric:emoval; train:0.8070; eval:0.6131; lr:0.000250
epoch:26; metric:emoval; train:0.8378; eval:0.5470; lr:0.000250
epoch:27; metric:emoval; train:0.8203; eval:0.6273; lr:0.000250
epoch:28; metric:emoval; train:0.8210; eval:0.6043; lr:0.000250
epoch:29; metric:emoval; train:0.8300; eval:0.6233; lr:0.000250
epoch:30; metric:emoval; train:0.8258; eval:0.6198; lr:0.000250
epoch:31; metric:emoval; train:0.8091; eval:0.5997; lr:0.000250
epoch:32; metric:emoval; train:0.8054; eval:0.5873; lr:0.000250
epoch:33; metric:emoval; train:0.7988; eval:0.5939; lr:0.000250
epoch:34; metric:emoval; train:0.8189; eval:0.6046; lr:0.000250
epoch:35; metric:emoval; train:0.8055; eval:0.5747; lr:0.000250
epoch:36; metric:emoval; train:0.7978; eval:0.6017; lr:0.000250
epoch:37; metric:emoval; train:0.8126; eval:0.5966; lr:0.000250
epoch:38; metric:emoval; train:0.7923; eval:0.6132; lr:0.000125
epoch:39; metric:emoval; train:0.8153; eval:0.6109; lr:0.000125
epoch:40; metric:emoval; train:0.8279; eval:0.5987; lr:0.000125
epoch:41; metric:emoval; train:0.8316; eval:0.6089; lr:0.000125
epoch:42; metric:emoval; train:0.8373; eval:0.6173; lr:0.000125
epoch:43; metric:emoval; train:0.8319; eval:0.6201; lr:0.000125
epoch:44; metric:emoval; train:0.8340; eval:0.6283; lr:0.000125
epoch:45; metric:emoval; train:0.8253; eval:0.6000; lr:0.000125
epoch:46; metric:emoval; train:0.8085; eval:0.5850; lr:0.000125
epoch:47; metric:emoval; train:0.8349; eval:0.6164; lr:0.000125
epoch:48; metric:emoval; train:0.8327; eval:0.6335; lr:0.000125
epoch:49; metric:emoval; train:0.8346; eval:0.6060; lr:0.000125
epoch:50; metric:emoval; train:0.8360; eval:0.5955; lr:0.000125
epoch:51; metric:emoval; train:0.8303; eval:0.5984; lr:0.000125
epoch:52; metric:emoval; train:0.8427; eval:0.5752; lr:0.000125
epoch:53; metric:emoval; train:0.8402; eval:0.6042; lr:0.000125
epoch:54; metric:emoval; train:0.8392; eval:0.6152; lr:0.000125
epoch:55; metric:emoval; train:0.8568; eval:0.5970; lr:0.000125
epoch:56; metric:emoval; train:0.8361; eval:0.6180; lr:0.000125
epoch:57; metric:emoval; train:0.8435; eval:0.6072; lr:0.000125
epoch:58; metric:emoval; train:0.8507; eval:0.6167; lr:0.000125
epoch:59; metric:emoval; train:0.8441; eval:0.6107; lr:0.000063
epoch:60; metric:emoval; train:0.8558; eval:0.6201; lr:0.000063
epoch:61; metric:emoval; train:0.8548; eval:0.6279; lr:0.000063
epoch:62; metric:emoval; train:0.8665; eval:0.6080; lr:0.000063
epoch:63; metric:emoval; train:0.8597; eval:0.6122; lr:0.000063
epoch:64; metric:emoval; train:0.8588; eval:0.6209; lr:0.000063
epoch:65; metric:emoval; train:0.8680; eval:0.6055; lr:0.000063
epoch:66; metric:emoval; train:0.8723; eval:0.6091; lr:0.000063
epoch:67; metric:emoval; train:0.8716; eval:0.5916; lr:0.000063
epoch:68; metric:emoval; train:0.8774; eval:0.6145; lr:0.000063
epoch:69; metric:emoval; train:0.8682; eval:0.6179; lr:0.000063
epoch:70; metric:emoval; train:0.8625; eval:0.6177; lr:0.000031
epoch:71; metric:emoval; train:0.8808; eval:0.6124; lr:0.000031
epoch:72; metric:emoval; train:0.8793; eval:0.6142; lr:0.000031
epoch:73; metric:emoval; train:0.8672; eval:0.6122; lr:0.000031
epoch:74; metric:emoval; train:0.8762; eval:0.6219; lr:0.000031
epoch:75; metric:emoval; train:0.8644; eval:0.6165; lr:0.000031
epoch:76; metric:emoval; train:0.8776; eval:0.6169; lr:0.000031
epoch:77; metric:emoval; train:0.8812; eval:0.6161; lr:0.000031
epoch:78; metric:emoval; train:0.8869; eval:0.6020; lr:0.000031
Early stopping at epoch 78, best epoch: 48
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 47, duration: 234.78768706321716 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0925; eval:0.2998; lr:0.000500
epoch:2; metric:emoval; train:0.3215; eval:0.3986; lr:0.000500
epoch:3; metric:emoval; train:0.4322; eval:0.4800; lr:0.000500
epoch:4; metric:emoval; train:0.5371; eval:0.4168; lr:0.000500
epoch:5; metric:emoval; train:0.5861; eval:0.5254; lr:0.000500
epoch:6; metric:emoval; train:0.6138; eval:0.5377; lr:0.000500
epoch:7; metric:emoval; train:0.6582; eval:0.5009; lr:0.000500
epoch:8; metric:emoval; train:0.6419; eval:0.4915; lr:0.000500
epoch:9; metric:emoval; train:0.6878; eval:0.5434; lr:0.000500
epoch:10; metric:emoval; train:0.7012; eval:0.5271; lr:0.000500
epoch:11; metric:emoval; train:0.6970; eval:0.5019; lr:0.000500
epoch:12; metric:emoval; train:0.7216; eval:0.5381; lr:0.000500
epoch:13; metric:emoval; train:0.7587; eval:0.5373; lr:0.000500
epoch:14; metric:emoval; train:0.7508; eval:0.5393; lr:0.000500
epoch:15; metric:emoval; train:0.7409; eval:0.5486; lr:0.000500
epoch:16; metric:emoval; train:0.7673; eval:0.5293; lr:0.000500
epoch:17; metric:emoval; train:0.7663; eval:0.5228; lr:0.000500
epoch:18; metric:emoval; train:0.7622; eval:0.5120; lr:0.000500
epoch:19; metric:emoval; train:0.7799; eval:0.5343; lr:0.000500
epoch:20; metric:emoval; train:0.7791; eval:0.5508; lr:0.000500
epoch:21; metric:emoval; train:0.7872; eval:0.5681; lr:0.000500
epoch:22; metric:emoval; train:0.7698; eval:0.4811; lr:0.000500
epoch:23; metric:emoval; train:0.7792; eval:0.5510; lr:0.000500
epoch:24; metric:emoval; train:0.7714; eval:0.5193; lr:0.000500
epoch:25; metric:emoval; train:0.7880; eval:0.5261; lr:0.000500
epoch:26; metric:emoval; train:0.7769; eval:0.5457; lr:0.000500
epoch:27; metric:emoval; train:0.7644; eval:0.4807; lr:0.000500
epoch:28; metric:emoval; train:0.7383; eval:0.5073; lr:0.000500
epoch:29; metric:emoval; train:0.7773; eval:0.5194; lr:0.000500
epoch:30; metric:emoval; train:0.7700; eval:0.5394; lr:0.000500
epoch:31; metric:emoval; train:0.7716; eval:0.4559; lr:0.000500
epoch:32; metric:emoval; train:0.7542; eval:0.5130; lr:0.000250
epoch:33; metric:emoval; train:0.8008; eval:0.5651; lr:0.000250
epoch:34; metric:emoval; train:0.8177; eval:0.5638; lr:0.000250
epoch:35; metric:emoval; train:0.8188; eval:0.5443; lr:0.000250
epoch:36; metric:emoval; train:0.8108; eval:0.5503; lr:0.000250
epoch:37; metric:emoval; train:0.8164; eval:0.5584; lr:0.000250
epoch:38; metric:emoval; train:0.8044; eval:0.5547; lr:0.000250
epoch:39; metric:emoval; train:0.8142; eval:0.5231; lr:0.000250
epoch:40; metric:emoval; train:0.8180; eval:0.5559; lr:0.000250
epoch:41; metric:emoval; train:0.8020; eval:0.5740; lr:0.000250
epoch:42; metric:emoval; train:0.8170; eval:0.5640; lr:0.000250
epoch:43; metric:emoval; train:0.8164; eval:0.5665; lr:0.000250
epoch:44; metric:emoval; train:0.8016; eval:0.5486; lr:0.000250
epoch:45; metric:emoval; train:0.7955; eval:0.5721; lr:0.000250
epoch:46; metric:emoval; train:0.7969; eval:0.5165; lr:0.000250
epoch:47; metric:emoval; train:0.8118; eval:0.5699; lr:0.000250
epoch:48; metric:emoval; train:0.8149; eval:0.5665; lr:0.000250
epoch:49; metric:emoval; train:0.8309; eval:0.5542; lr:0.000250
epoch:50; metric:emoval; train:0.8100; eval:0.5427; lr:0.000250
epoch:51; metric:emoval; train:0.7985; eval:0.5423; lr:0.000250
epoch:52; metric:emoval; train:0.8214; eval:0.5668; lr:0.000125
epoch:53; metric:emoval; train:0.8356; eval:0.5382; lr:0.000125
epoch:54; metric:emoval; train:0.8493; eval:0.5734; lr:0.000125
epoch:55; metric:emoval; train:0.8282; eval:0.5727; lr:0.000125
epoch:56; metric:emoval; train:0.8403; eval:0.5714; lr:0.000125
epoch:57; metric:emoval; train:0.8474; eval:0.5625; lr:0.000125
epoch:58; metric:emoval; train:0.8591; eval:0.5539; lr:0.000125
epoch:59; metric:emoval; train:0.8604; eval:0.5537; lr:0.000125
epoch:60; metric:emoval; train:0.8609; eval:0.5556; lr:0.000125
epoch:61; metric:emoval; train:0.8516; eval:0.5709; lr:0.000125
epoch:62; metric:emoval; train:0.8502; eval:0.5994; lr:0.000125
epoch:63; metric:emoval; train:0.8479; eval:0.5619; lr:0.000125
epoch:64; metric:emoval; train:0.8487; eval:0.5356; lr:0.000125
epoch:65; metric:emoval; train:0.8524; eval:0.5695; lr:0.000125
epoch:66; metric:emoval; train:0.8551; eval:0.5547; lr:0.000125
epoch:67; metric:emoval; train:0.8696; eval:0.5743; lr:0.000125
epoch:68; metric:emoval; train:0.8535; eval:0.5694; lr:0.000125
epoch:69; metric:emoval; train:0.8407; eval:0.5497; lr:0.000125
epoch:70; metric:emoval; train:0.8534; eval:0.5684; lr:0.000125
epoch:71; metric:emoval; train:0.8679; eval:0.5611; lr:0.000125
epoch:72; metric:emoval; train:0.8529; eval:0.5600; lr:0.000125
epoch:73; metric:emoval; train:0.8733; eval:0.5708; lr:0.000063
epoch:74; metric:emoval; train:0.8779; eval:0.5572; lr:0.000063
epoch:75; metric:emoval; train:0.8742; eval:0.5652; lr:0.000063
epoch:76; metric:emoval; train:0.8650; eval:0.5790; lr:0.000063
epoch:77; metric:emoval; train:0.8672; eval:0.5678; lr:0.000063
epoch:78; metric:emoval; train:0.8754; eval:0.5863; lr:0.000063
epoch:79; metric:emoval; train:0.8682; eval:0.5582; lr:0.000063
epoch:80; metric:emoval; train:0.8739; eval:0.5916; lr:0.000063
epoch:81; metric:emoval; train:0.8898; eval:0.5702; lr:0.000063
epoch:82; metric:emoval; train:0.8816; eval:0.5667; lr:0.000063
epoch:83; metric:emoval; train:0.8788; eval:0.5703; lr:0.000063
epoch:84; metric:emoval; train:0.8654; eval:0.5657; lr:0.000031
epoch:85; metric:emoval; train:0.8928; eval:0.5629; lr:0.000031
epoch:86; metric:emoval; train:0.8866; eval:0.5720; lr:0.000031
epoch:87; metric:emoval; train:0.8799; eval:0.5842; lr:0.000031
epoch:88; metric:emoval; train:0.8918; eval:0.5947; lr:0.000031
epoch:89; metric:emoval; train:0.8894; eval:0.5886; lr:0.000031
epoch:90; metric:emoval; train:0.8902; eval:0.5843; lr:0.000031
epoch:91; metric:emoval; train:0.8995; eval:0.5823; lr:0.000031
epoch:92; metric:emoval; train:0.8833; eval:0.5772; lr:0.000031
Early stopping at epoch 92, best epoch: 62
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 61, duration: 276.24895000457764 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0960; eval:0.3960; lr:0.000500
epoch:2; metric:emoval; train:0.3053; eval:0.2671; lr:0.000500
epoch:3; metric:emoval; train:0.4415; eval:0.4204; lr:0.000500
epoch:4; metric:emoval; train:0.4735; eval:0.5419; lr:0.000500
epoch:5; metric:emoval; train:0.5642; eval:0.4262; lr:0.000500
epoch:6; metric:emoval; train:0.5947; eval:0.5395; lr:0.000500
epoch:7; metric:emoval; train:0.6403; eval:0.5833; lr:0.000500
epoch:8; metric:emoval; train:0.6476; eval:0.5701; lr:0.000500
epoch:9; metric:emoval; train:0.6722; eval:0.4405; lr:0.000500
epoch:10; metric:emoval; train:0.6867; eval:0.5449; lr:0.000500
epoch:11; metric:emoval; train:0.7130; eval:0.5449; lr:0.000500
epoch:12; metric:emoval; train:0.7188; eval:0.5931; lr:0.000500
epoch:13; metric:emoval; train:0.7321; eval:0.5513; lr:0.000500
epoch:14; metric:emoval; train:0.7162; eval:0.5716; lr:0.000500
epoch:15; metric:emoval; train:0.7208; eval:0.5660; lr:0.000500
epoch:16; metric:emoval; train:0.7412; eval:0.5983; lr:0.000500
epoch:17; metric:emoval; train:0.7610; eval:0.5612; lr:0.000500
epoch:18; metric:emoval; train:0.7704; eval:0.5679; lr:0.000500
epoch:19; metric:emoval; train:0.7693; eval:0.5811; lr:0.000500
epoch:20; metric:emoval; train:0.7629; eval:0.5932; lr:0.000500
epoch:21; metric:emoval; train:0.7773; eval:0.5873; lr:0.000500
epoch:22; metric:emoval; train:0.7856; eval:0.5761; lr:0.000500
epoch:23; metric:emoval; train:0.7663; eval:0.5724; lr:0.000500
epoch:24; metric:emoval; train:0.7609; eval:0.5997; lr:0.000500
epoch:25; metric:emoval; train:0.7778; eval:0.5845; lr:0.000500
epoch:26; metric:emoval; train:0.7752; eval:0.5666; lr:0.000500
epoch:27; metric:emoval; train:0.7799; eval:0.5927; lr:0.000500
epoch:28; metric:emoval; train:0.7677; eval:0.6081; lr:0.000500
epoch:29; metric:emoval; train:0.7577; eval:0.5891; lr:0.000500
epoch:30; metric:emoval; train:0.7525; eval:0.5915; lr:0.000500
epoch:31; metric:emoval; train:0.7872; eval:0.5663; lr:0.000500
epoch:32; metric:emoval; train:0.7692; eval:0.5587; lr:0.000500
epoch:33; metric:emoval; train:0.7643; eval:0.4935; lr:0.000500
epoch:34; metric:emoval; train:0.7534; eval:0.5759; lr:0.000500
epoch:35; metric:emoval; train:0.7489; eval:0.5650; lr:0.000500
epoch:36; metric:emoval; train:0.7573; eval:0.5799; lr:0.000500
epoch:37; metric:emoval; train:0.7345; eval:0.5381; lr:0.000500
epoch:38; metric:emoval; train:0.7475; eval:0.5051; lr:0.000500
epoch:39; metric:emoval; train:0.7437; eval:0.5611; lr:0.000250
epoch:40; metric:emoval; train:0.8036; eval:0.5978; lr:0.000250
epoch:41; metric:emoval; train:0.7984; eval:0.5929; lr:0.000250
epoch:42; metric:emoval; train:0.7985; eval:0.6072; lr:0.000250
epoch:43; metric:emoval; train:0.8073; eval:0.6025; lr:0.000250
epoch:44; metric:emoval; train:0.8004; eval:0.6177; lr:0.000250
epoch:45; metric:emoval; train:0.8150; eval:0.6252; lr:0.000250
epoch:46; metric:emoval; train:0.8008; eval:0.6078; lr:0.000250
epoch:47; metric:emoval; train:0.8016; eval:0.6338; lr:0.000250
epoch:48; metric:emoval; train:0.8164; eval:0.5936; lr:0.000250
epoch:49; metric:emoval; train:0.8197; eval:0.5911; lr:0.000250
epoch:50; metric:emoval; train:0.8151; eval:0.5992; lr:0.000250
epoch:51; metric:emoval; train:0.7980; eval:0.5940; lr:0.000250
epoch:52; metric:emoval; train:0.8066; eval:0.5992; lr:0.000250
epoch:53; metric:emoval; train:0.8216; eval:0.6064; lr:0.000250
epoch:54; metric:emoval; train:0.8176; eval:0.5904; lr:0.000250
epoch:55; metric:emoval; train:0.7985; eval:0.5751; lr:0.000250
epoch:56; metric:emoval; train:0.8056; eval:0.5815; lr:0.000250
epoch:57; metric:emoval; train:0.8180; eval:0.6033; lr:0.000250
epoch:58; metric:emoval; train:0.8254; eval:0.5689; lr:0.000125
epoch:59; metric:emoval; train:0.8423; eval:0.6049; lr:0.000125
epoch:60; metric:emoval; train:0.8395; eval:0.6042; lr:0.000125
epoch:61; metric:emoval; train:0.8569; eval:0.6209; lr:0.000125
epoch:62; metric:emoval; train:0.8527; eval:0.6187; lr:0.000125
epoch:63; metric:emoval; train:0.8525; eval:0.6395; lr:0.000125
epoch:64; metric:emoval; train:0.8518; eval:0.6089; lr:0.000125
epoch:65; metric:emoval; train:0.8625; eval:0.5961; lr:0.000125
epoch:66; metric:emoval; train:0.8597; eval:0.6165; lr:0.000125
epoch:67; metric:emoval; train:0.8497; eval:0.5878; lr:0.000125
epoch:68; metric:emoval; train:0.8496; eval:0.6116; lr:0.000125
epoch:69; metric:emoval; train:0.8367; eval:0.5775; lr:0.000125
epoch:70; metric:emoval; train:0.8539; eval:0.5884; lr:0.000125
epoch:71; metric:emoval; train:0.8594; eval:0.5987; lr:0.000125
epoch:72; metric:emoval; train:0.8577; eval:0.6325; lr:0.000125
epoch:73; metric:emoval; train:0.8612; eval:0.6058; lr:0.000125
epoch:74; metric:emoval; train:0.8513; eval:0.5900; lr:0.000063
epoch:75; metric:emoval; train:0.8705; eval:0.6022; lr:0.000063
epoch:76; metric:emoval; train:0.8650; eval:0.6114; lr:0.000063
epoch:77; metric:emoval; train:0.8632; eval:0.6095; lr:0.000063
epoch:78; metric:emoval; train:0.8782; eval:0.5923; lr:0.000063
epoch:79; metric:emoval; train:0.8770; eval:0.6108; lr:0.000063
epoch:80; metric:emoval; train:0.8717; eval:0.6158; lr:0.000063
epoch:81; metric:emoval; train:0.8719; eval:0.6055; lr:0.000063
epoch:82; metric:emoval; train:0.8827; eval:0.6141; lr:0.000063
epoch:83; metric:emoval; train:0.8736; eval:0.5932; lr:0.000063
epoch:84; metric:emoval; train:0.8844; eval:0.6056; lr:0.000063
epoch:85; metric:emoval; train:0.8791; eval:0.5930; lr:0.000031
epoch:86; metric:emoval; train:0.8924; eval:0.6142; lr:0.000031
epoch:87; metric:emoval; train:0.8974; eval:0.6052; lr:0.000031
epoch:88; metric:emoval; train:0.8839; eval:0.6046; lr:0.000031
epoch:89; metric:emoval; train:0.8900; eval:0.6069; lr:0.000031
epoch:90; metric:emoval; train:0.8856; eval:0.6151; lr:0.000031
epoch:91; metric:emoval; train:0.8906; eval:0.6150; lr:0.000031
epoch:92; metric:emoval; train:0.8853; eval:0.6104; lr:0.000031
epoch:93; metric:emoval; train:0.8806; eval:0.6104; lr:0.000031
Early stopping at epoch 93, best epoch: 63
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 62, duration: 278.88742089271545 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0813; eval:0.4695; lr:0.000500
epoch:2; metric:emoval; train:0.3236; eval:0.5373; lr:0.000500
epoch:3; metric:emoval; train:0.4340; eval:0.5628; lr:0.000500
epoch:4; metric:emoval; train:0.4972; eval:0.5349; lr:0.000500
epoch:5; metric:emoval; train:0.5681; eval:0.5593; lr:0.000500
epoch:6; metric:emoval; train:0.6033; eval:0.5408; lr:0.000500
epoch:7; metric:emoval; train:0.6247; eval:0.5460; lr:0.000500
epoch:8; metric:emoval; train:0.6470; eval:0.5777; lr:0.000500
epoch:9; metric:emoval; train:0.6685; eval:0.5768; lr:0.000500
epoch:10; metric:emoval; train:0.6817; eval:0.4249; lr:0.000500
epoch:11; metric:emoval; train:0.7053; eval:0.5522; lr:0.000500
epoch:12; metric:emoval; train:0.7093; eval:0.5505; lr:0.000500
epoch:13; metric:emoval; train:0.7276; eval:0.5416; lr:0.000500
epoch:14; metric:emoval; train:0.7276; eval:0.5889; lr:0.000500
epoch:15; metric:emoval; train:0.7407; eval:0.5892; lr:0.000500
epoch:16; metric:emoval; train:0.7528; eval:0.5983; lr:0.000500
epoch:17; metric:emoval; train:0.7567; eval:0.5405; lr:0.000500
epoch:18; metric:emoval; train:0.7505; eval:0.5626; lr:0.000500
epoch:19; metric:emoval; train:0.7715; eval:0.5484; lr:0.000500
epoch:20; metric:emoval; train:0.7912; eval:0.5928; lr:0.000500
epoch:21; metric:emoval; train:0.7830; eval:0.5902; lr:0.000500
epoch:22; metric:emoval; train:0.7713; eval:0.5556; lr:0.000500
epoch:23; metric:emoval; train:0.7754; eval:0.5890; lr:0.000500
epoch:24; metric:emoval; train:0.7591; eval:0.5830; lr:0.000500
epoch:25; metric:emoval; train:0.7474; eval:0.5695; lr:0.000500
epoch:26; metric:emoval; train:0.7793; eval:0.5677; lr:0.000500
epoch:27; metric:emoval; train:0.7780; eval:0.5678; lr:0.000250
epoch:28; metric:emoval; train:0.8028; eval:0.5738; lr:0.000250
epoch:29; metric:emoval; train:0.8168; eval:0.5832; lr:0.000250
epoch:30; metric:emoval; train:0.8219; eval:0.5358; lr:0.000250
epoch:31; metric:emoval; train:0.8303; eval:0.5939; lr:0.000250
epoch:32; metric:emoval; train:0.8252; eval:0.6124; lr:0.000250
epoch:33; metric:emoval; train:0.8189; eval:0.5770; lr:0.000250
epoch:34; metric:emoval; train:0.7967; eval:0.6151; lr:0.000250
epoch:35; metric:emoval; train:0.8140; eval:0.5431; lr:0.000250
epoch:36; metric:emoval; train:0.7804; eval:0.6081; lr:0.000250
epoch:37; metric:emoval; train:0.8135; eval:0.5897; lr:0.000250
epoch:38; metric:emoval; train:0.7968; eval:0.5421; lr:0.000250
epoch:39; metric:emoval; train:0.7858; eval:0.5947; lr:0.000250
epoch:40; metric:emoval; train:0.8099; eval:0.6078; lr:0.000250
epoch:41; metric:emoval; train:0.8043; eval:0.5875; lr:0.000250
epoch:42; metric:emoval; train:0.8074; eval:0.5898; lr:0.000250
epoch:43; metric:emoval; train:0.8150; eval:0.6042; lr:0.000250
epoch:44; metric:emoval; train:0.8105; eval:0.5816; lr:0.000250
epoch:45; metric:emoval; train:0.7932; eval:0.5615; lr:0.000125
epoch:46; metric:emoval; train:0.8199; eval:0.5978; lr:0.000125
epoch:47; metric:emoval; train:0.8345; eval:0.5699; lr:0.000125
epoch:48; metric:emoval; train:0.8191; eval:0.6013; lr:0.000125
epoch:49; metric:emoval; train:0.8153; eval:0.5908; lr:0.000125
epoch:50; metric:emoval; train:0.8318; eval:0.5855; lr:0.000125
epoch:51; metric:emoval; train:0.8421; eval:0.5964; lr:0.000125
epoch:52; metric:emoval; train:0.8366; eval:0.5834; lr:0.000125
epoch:53; metric:emoval; train:0.8544; eval:0.6174; lr:0.000125
epoch:54; metric:emoval; train:0.8489; eval:0.5695; lr:0.000125
epoch:55; metric:emoval; train:0.8430; eval:0.6249; lr:0.000125
epoch:56; metric:emoval; train:0.8518; eval:0.6042; lr:0.000125
epoch:57; metric:emoval; train:0.8466; eval:0.5967; lr:0.000125
epoch:58; metric:emoval; train:0.8472; eval:0.6125; lr:0.000125
epoch:59; metric:emoval; train:0.8349; eval:0.6142; lr:0.000125
epoch:60; metric:emoval; train:0.8482; eval:0.6073; lr:0.000125
epoch:61; metric:emoval; train:0.8400; eval:0.6026; lr:0.000125
epoch:62; metric:emoval; train:0.8444; eval:0.5991; lr:0.000125
epoch:63; metric:emoval; train:0.8626; eval:0.5843; lr:0.000125
epoch:64; metric:emoval; train:0.8445; eval:0.5752; lr:0.000125
epoch:65; metric:emoval; train:0.8479; eval:0.6205; lr:0.000125
epoch:66; metric:emoval; train:0.8608; eval:0.5985; lr:0.000063
epoch:67; metric:emoval; train:0.8571; eval:0.6272; lr:0.000063
epoch:68; metric:emoval; train:0.8685; eval:0.6186; lr:0.000063
epoch:69; metric:emoval; train:0.8658; eval:0.6031; lr:0.000063
epoch:70; metric:emoval; train:0.8730; eval:0.6167; lr:0.000063
epoch:71; metric:emoval; train:0.8626; eval:0.6092; lr:0.000063
epoch:72; metric:emoval; train:0.8745; eval:0.5978; lr:0.000063
epoch:73; metric:emoval; train:0.8721; eval:0.6169; lr:0.000063
epoch:74; metric:emoval; train:0.8800; eval:0.5805; lr:0.000063
epoch:75; metric:emoval; train:0.8748; eval:0.5930; lr:0.000063
epoch:76; metric:emoval; train:0.8716; eval:0.6020; lr:0.000063
epoch:77; metric:emoval; train:0.8822; eval:0.6124; lr:0.000063
epoch:78; metric:emoval; train:0.8831; eval:0.6371; lr:0.000063
epoch:79; metric:emoval; train:0.8911; eval:0.6157; lr:0.000063
epoch:80; metric:emoval; train:0.8720; eval:0.6135; lr:0.000063
epoch:81; metric:emoval; train:0.8783; eval:0.6116; lr:0.000063
epoch:82; metric:emoval; train:0.8701; eval:0.6198; lr:0.000063
epoch:83; metric:emoval; train:0.8734; eval:0.6107; lr:0.000063
epoch:84; metric:emoval; train:0.8765; eval:0.6090; lr:0.000063
epoch:85; metric:emoval; train:0.8802; eval:0.6126; lr:0.000063
epoch:86; metric:emoval; train:0.8732; eval:0.6012; lr:0.000063
epoch:87; metric:emoval; train:0.8669; eval:0.6144; lr:0.000063
epoch:88; metric:emoval; train:0.8740; eval:0.6172; lr:0.000063
epoch:89; metric:emoval; train:0.8713; eval:0.6200; lr:0.000031
epoch:90; metric:emoval; train:0.8881; eval:0.6077; lr:0.000031
epoch:91; metric:emoval; train:0.8961; eval:0.6212; lr:0.000031
epoch:92; metric:emoval; train:0.8864; eval:0.5974; lr:0.000031
epoch:93; metric:emoval; train:0.8922; eval:0.6139; lr:0.000031
epoch:94; metric:emoval; train:0.8847; eval:0.6170; lr:0.000031
epoch:95; metric:emoval; train:0.8843; eval:0.6337; lr:0.000031
epoch:96; metric:emoval; train:0.8900; eval:0.6172; lr:0.000031
epoch:97; metric:emoval; train:0.8938; eval:0.6291; lr:0.000031
epoch:98; metric:emoval; train:0.8902; eval:0.6054; lr:0.000031
epoch:99; metric:emoval; train:0.8857; eval:0.6229; lr:0.000031
epoch:100; metric:emoval; train:0.8895; eval:0.5993; lr:0.000016
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 77, duration: 270.60235118865967 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.7707_acc:0.7720_val:0.5787_1770110934.9007328.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.8103_acc:0.8102_val:0.6458_1770110934.9007328.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.7670_acc:0.7694_val:0.6293_1770110934.9007328.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.8897_acc:0.8909_val:80.3369_1770110934.9007328.npz
