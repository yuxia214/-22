====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v4', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_gated_fusion=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 34%|███▍      | 1142/3373 [00:00<00:00, 11342.67it/s] 68%|██████▊   | 2277/3373 [00:00<00:00, 9501.65it/s]  99%|█████████▉| 3338/3373 [00:00<00:00, 9888.78it/s]100%|██████████| 3373/3373 [00:00<00:00, 10039.49it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 26%|██▌       | 875/3373 [00:00<00:00, 8735.72it/s] 52%|█████▏    | 1749/3373 [00:00<00:00, 8718.36it/s] 78%|███████▊  | 2621/3373 [00:00<00:00, 8409.49it/s]100%|██████████| 3373/3373 [00:00<00:00, 8713.43it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 46%|████▌     | 1537/3373 [00:00<00:00, 15361.15it/s] 91%|█████████ | 3074/3373 [00:00<00:00, 13146.80it/s]100%|██████████| 3373/3373 [00:00<00:00, 13813.18it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 7643.96it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 11835.14it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 13811.42it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 13635.92it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 10064.79it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 14686.96it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 12125.75it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 10535.85it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14102.31it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1563; eval:0.3210; lr:0.000500
epoch:2; metric:emoval; train:0.2983; eval:0.4694; lr:0.000500
epoch:3; metric:emoval; train:0.4268; eval:0.4921; lr:0.000500
epoch:4; metric:emoval; train:0.4783; eval:0.4424; lr:0.000500
epoch:5; metric:emoval; train:0.5678; eval:0.5218; lr:0.000500
epoch:6; metric:emoval; train:0.5734; eval:0.5352; lr:0.000500
epoch:7; metric:emoval; train:0.6210; eval:0.4859; lr:0.000500
epoch:8; metric:emoval; train:0.6510; eval:0.4878; lr:0.000500
epoch:9; metric:emoval; train:0.6829; eval:0.4461; lr:0.000500
epoch:10; metric:emoval; train:0.6846; eval:0.5648; lr:0.000500
epoch:11; metric:emoval; train:0.6953; eval:0.4887; lr:0.000500
epoch:12; metric:emoval; train:0.7097; eval:0.5127; lr:0.000500
epoch:13; metric:emoval; train:0.7273; eval:0.5073; lr:0.000500
epoch:14; metric:emoval; train:0.7368; eval:0.5339; lr:0.000500
epoch:15; metric:emoval; train:0.7720; eval:0.5309; lr:0.000500
epoch:16; metric:emoval; train:0.7409; eval:0.5403; lr:0.000500
epoch:17; metric:emoval; train:0.7541; eval:0.5154; lr:0.000500
epoch:18; metric:emoval; train:0.7611; eval:0.5410; lr:0.000500
epoch:19; metric:emoval; train:0.7671; eval:0.5629; lr:0.000500
epoch:20; metric:emoval; train:0.7671; eval:0.5513; lr:0.000500
epoch:21; metric:emoval; train:0.7894; eval:0.5328; lr:0.000250
epoch:22; metric:emoval; train:0.8179; eval:0.5657; lr:0.000250
epoch:23; metric:emoval; train:0.8216; eval:0.5726; lr:0.000250
epoch:24; metric:emoval; train:0.8397; eval:0.5512; lr:0.000250
epoch:25; metric:emoval; train:0.8191; eval:0.5647; lr:0.000250
epoch:26; metric:emoval; train:0.8322; eval:0.5523; lr:0.000250
epoch:27; metric:emoval; train:0.8072; eval:0.5378; lr:0.000250
epoch:28; metric:emoval; train:0.8172; eval:0.5704; lr:0.000250
epoch:29; metric:emoval; train:0.8221; eval:0.5876; lr:0.000250
epoch:30; metric:emoval; train:0.8159; eval:0.5549; lr:0.000250
epoch:31; metric:emoval; train:0.8070; eval:0.5417; lr:0.000250
epoch:32; metric:emoval; train:0.8039; eval:0.5702; lr:0.000250
epoch:33; metric:emoval; train:0.8032; eval:0.5690; lr:0.000250
epoch:34; metric:emoval; train:0.8037; eval:0.5614; lr:0.000250
epoch:35; metric:emoval; train:0.8011; eval:0.5324; lr:0.000250
epoch:36; metric:emoval; train:0.7913; eval:0.5418; lr:0.000250
epoch:37; metric:emoval; train:0.7817; eval:0.5680; lr:0.000250
epoch:38; metric:emoval; train:0.7958; eval:0.5785; lr:0.000250
epoch:39; metric:emoval; train:0.8118; eval:0.5771; lr:0.000250
epoch:40; metric:emoval; train:0.7781; eval:0.5360; lr:0.000125
epoch:41; metric:emoval; train:0.8206; eval:0.5590; lr:0.000125
epoch:42; metric:emoval; train:0.8164; eval:0.5747; lr:0.000125
epoch:43; metric:emoval; train:0.8301; eval:0.5485; lr:0.000125
epoch:44; metric:emoval; train:0.8326; eval:0.5547; lr:0.000125
epoch:45; metric:emoval; train:0.8322; eval:0.5593; lr:0.000125
epoch:46; metric:emoval; train:0.8402; eval:0.5452; lr:0.000125
epoch:47; metric:emoval; train:0.8285; eval:0.5768; lr:0.000125
epoch:48; metric:emoval; train:0.8330; eval:0.5814; lr:0.000125
epoch:49; metric:emoval; train:0.8294; eval:0.5688; lr:0.000125
epoch:50; metric:emoval; train:0.8337; eval:0.5798; lr:0.000125
epoch:51; metric:emoval; train:0.8270; eval:0.5854; lr:0.000063
epoch:52; metric:emoval; train:0.8516; eval:0.5769; lr:0.000063
epoch:53; metric:emoval; train:0.8682; eval:0.5870; lr:0.000063
epoch:54; metric:emoval; train:0.8532; eval:0.5872; lr:0.000063
epoch:55; metric:emoval; train:0.8513; eval:0.5818; lr:0.000063
epoch:56; metric:emoval; train:0.8582; eval:0.5856; lr:0.000063
epoch:57; metric:emoval; train:0.8527; eval:0.5882; lr:0.000063
epoch:58; metric:emoval; train:0.8690; eval:0.5721; lr:0.000063
epoch:59; metric:emoval; train:0.8554; eval:0.5932; lr:0.000063
epoch:60; metric:emoval; train:0.8618; eval:0.6021; lr:0.000063
epoch:61; metric:emoval; train:0.8606; eval:0.5617; lr:0.000063
epoch:62; metric:emoval; train:0.8533; eval:0.5659; lr:0.000063
epoch:63; metric:emoval; train:0.8612; eval:0.5885; lr:0.000063
epoch:64; metric:emoval; train:0.8580; eval:0.5777; lr:0.000063
epoch:65; metric:emoval; train:0.8623; eval:0.5917; lr:0.000063
epoch:66; metric:emoval; train:0.8638; eval:0.5849; lr:0.000063
epoch:67; metric:emoval; train:0.8546; eval:0.5827; lr:0.000063
epoch:68; metric:emoval; train:0.8576; eval:0.5589; lr:0.000063
epoch:69; metric:emoval; train:0.8567; eval:0.5620; lr:0.000063
epoch:70; metric:emoval; train:0.8584; eval:0.5867; lr:0.000063
epoch:71; metric:emoval; train:0.8615; eval:0.5802; lr:0.000031
epoch:72; metric:emoval; train:0.8783; eval:0.5988; lr:0.000031
epoch:73; metric:emoval; train:0.8774; eval:0.5823; lr:0.000031
epoch:74; metric:emoval; train:0.8669; eval:0.5989; lr:0.000031
epoch:75; metric:emoval; train:0.8678; eval:0.5877; lr:0.000031
epoch:76; metric:emoval; train:0.8799; eval:0.5968; lr:0.000031
epoch:77; metric:emoval; train:0.8719; eval:0.6056; lr:0.000031
epoch:78; metric:emoval; train:0.8791; eval:0.5791; lr:0.000031
epoch:79; metric:emoval; train:0.8703; eval:0.5847; lr:0.000031
epoch:80; metric:emoval; train:0.8816; eval:0.5910; lr:0.000031
epoch:81; metric:emoval; train:0.8778; eval:0.5900; lr:0.000031
epoch:82; metric:emoval; train:0.8827; eval:0.5920; lr:0.000031
epoch:83; metric:emoval; train:0.8764; eval:0.5921; lr:0.000031
epoch:84; metric:emoval; train:0.8812; eval:0.5824; lr:0.000031
epoch:85; metric:emoval; train:0.8876; eval:0.5678; lr:0.000031
epoch:86; metric:emoval; train:0.8626; eval:0.5884; lr:0.000031
epoch:87; metric:emoval; train:0.8770; eval:0.5749; lr:0.000031
epoch:88; metric:emoval; train:0.8741; eval:0.5926; lr:0.000016
epoch:89; metric:emoval; train:0.8761; eval:0.5997; lr:0.000016
epoch:90; metric:emoval; train:0.8787; eval:0.5908; lr:0.000016
epoch:91; metric:emoval; train:0.8810; eval:0.5942; lr:0.000016
epoch:92; metric:emoval; train:0.8973; eval:0.5989; lr:0.000016
epoch:93; metric:emoval; train:0.8652; eval:0.5949; lr:0.000016
epoch:94; metric:emoval; train:0.8764; eval:0.5895; lr:0.000016
epoch:95; metric:emoval; train:0.8850; eval:0.5830; lr:0.000016
epoch:96; metric:emoval; train:0.8862; eval:0.5898; lr:0.000016
epoch:97; metric:emoval; train:0.8970; eval:0.5876; lr:0.000016
epoch:98; metric:emoval; train:0.8872; eval:0.5971; lr:0.000016
epoch:99; metric:emoval; train:0.8833; eval:0.5897; lr:0.000008
epoch:100; metric:emoval; train:0.8875; eval:0.5902; lr:0.000008
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 76, duration: 337.0111846923828 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1483; eval:0.2950; lr:0.000500
epoch:2; metric:emoval; train:0.3030; eval:0.4160; lr:0.000500
epoch:3; metric:emoval; train:0.3990; eval:0.3830; lr:0.000500
epoch:4; metric:emoval; train:0.4653; eval:0.4324; lr:0.000500
epoch:5; metric:emoval; train:0.5418; eval:0.5056; lr:0.000500
epoch:6; metric:emoval; train:0.5893; eval:0.5401; lr:0.000500
epoch:7; metric:emoval; train:0.6073; eval:0.5541; lr:0.000500
epoch:8; metric:emoval; train:0.6370; eval:0.5942; lr:0.000500
epoch:9; metric:emoval; train:0.6892; eval:0.5872; lr:0.000500
epoch:10; metric:emoval; train:0.6785; eval:0.5969; lr:0.000500
epoch:11; metric:emoval; train:0.6874; eval:0.5749; lr:0.000500
epoch:12; metric:emoval; train:0.7248; eval:0.5966; lr:0.000500
epoch:13; metric:emoval; train:0.7163; eval:0.5577; lr:0.000500
epoch:14; metric:emoval; train:0.7243; eval:0.5997; lr:0.000500
epoch:15; metric:emoval; train:0.7415; eval:0.5834; lr:0.000500
epoch:16; metric:emoval; train:0.7439; eval:0.5390; lr:0.000500
epoch:17; metric:emoval; train:0.7441; eval:0.5433; lr:0.000500
epoch:18; metric:emoval; train:0.7554; eval:0.5524; lr:0.000500
epoch:19; metric:emoval; train:0.7541; eval:0.5660; lr:0.000500
epoch:20; metric:emoval; train:0.7686; eval:0.5407; lr:0.000500
epoch:21; metric:emoval; train:0.7665; eval:0.5324; lr:0.000500
epoch:22; metric:emoval; train:0.7563; eval:0.5457; lr:0.000500
epoch:23; metric:emoval; train:0.7683; eval:0.5699; lr:0.000500
epoch:24; metric:emoval; train:0.7760; eval:0.5959; lr:0.000500
epoch:25; metric:emoval; train:0.7755; eval:0.5909; lr:0.000250
epoch:26; metric:emoval; train:0.8010; eval:0.5919; lr:0.000250
epoch:27; metric:emoval; train:0.8291; eval:0.6116; lr:0.000250
epoch:28; metric:emoval; train:0.8288; eval:0.5954; lr:0.000250
epoch:29; metric:emoval; train:0.8110; eval:0.5615; lr:0.000250
epoch:30; metric:emoval; train:0.8097; eval:0.5900; lr:0.000250
epoch:31; metric:emoval; train:0.8013; eval:0.5798; lr:0.000250
epoch:32; metric:emoval; train:0.8209; eval:0.5585; lr:0.000250
epoch:33; metric:emoval; train:0.8069; eval:0.5981; lr:0.000250
epoch:34; metric:emoval; train:0.8081; eval:0.5876; lr:0.000250
epoch:35; metric:emoval; train:0.8154; eval:0.5813; lr:0.000250
epoch:36; metric:emoval; train:0.7976; eval:0.5848; lr:0.000250
epoch:37; metric:emoval; train:0.7994; eval:0.5790; lr:0.000250
epoch:38; metric:emoval; train:0.7974; eval:0.6079; lr:0.000125
epoch:39; metric:emoval; train:0.8104; eval:0.6070; lr:0.000125
epoch:40; metric:emoval; train:0.8084; eval:0.5951; lr:0.000125
epoch:41; metric:emoval; train:0.8020; eval:0.5860; lr:0.000125
epoch:42; metric:emoval; train:0.8291; eval:0.5972; lr:0.000125
epoch:43; metric:emoval; train:0.8410; eval:0.5948; lr:0.000125
epoch:44; metric:emoval; train:0.8322; eval:0.6261; lr:0.000125
epoch:45; metric:emoval; train:0.8217; eval:0.5878; lr:0.000125
epoch:46; metric:emoval; train:0.8391; eval:0.6104; lr:0.000125
epoch:47; metric:emoval; train:0.8453; eval:0.6042; lr:0.000125
epoch:48; metric:emoval; train:0.8337; eval:0.5968; lr:0.000125
epoch:49; metric:emoval; train:0.8357; eval:0.6038; lr:0.000125
epoch:50; metric:emoval; train:0.8393; eval:0.5877; lr:0.000125
epoch:51; metric:emoval; train:0.8297; eval:0.5859; lr:0.000125
epoch:52; metric:emoval; train:0.8377; eval:0.5999; lr:0.000125
epoch:53; metric:emoval; train:0.8444; eval:0.5981; lr:0.000125
epoch:54; metric:emoval; train:0.8413; eval:0.6041; lr:0.000125
epoch:55; metric:emoval; train:0.8141; eval:0.5867; lr:0.000063
epoch:56; metric:emoval; train:0.8504; eval:0.6114; lr:0.000063
epoch:57; metric:emoval; train:0.8526; eval:0.6082; lr:0.000063
epoch:58; metric:emoval; train:0.8576; eval:0.6100; lr:0.000063
epoch:59; metric:emoval; train:0.8623; eval:0.6014; lr:0.000063
epoch:60; metric:emoval; train:0.8612; eval:0.6190; lr:0.000063
epoch:61; metric:emoval; train:0.8529; eval:0.6164; lr:0.000063
epoch:62; metric:emoval; train:0.8704; eval:0.6046; lr:0.000063
epoch:63; metric:emoval; train:0.8552; eval:0.6095; lr:0.000063
epoch:64; metric:emoval; train:0.8498; eval:0.6122; lr:0.000063
epoch:65; metric:emoval; train:0.8541; eval:0.5818; lr:0.000063
epoch:66; metric:emoval; train:0.8761; eval:0.6037; lr:0.000031
epoch:67; metric:emoval; train:0.8699; eval:0.6182; lr:0.000031
epoch:68; metric:emoval; train:0.8738; eval:0.6048; lr:0.000031
epoch:69; metric:emoval; train:0.8714; eval:0.6148; lr:0.000031
epoch:70; metric:emoval; train:0.8567; eval:0.6127; lr:0.000031
epoch:71; metric:emoval; train:0.8759; eval:0.6116; lr:0.000031
epoch:72; metric:emoval; train:0.8847; eval:0.6156; lr:0.000031
epoch:73; metric:emoval; train:0.8689; eval:0.6161; lr:0.000031
epoch:74; metric:emoval; train:0.8687; eval:0.6250; lr:0.000031
Early stopping at epoch 74, best epoch: 44
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 43, duration: 243.75794172286987 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1092; eval:0.3785; lr:0.000500
epoch:2; metric:emoval; train:0.3061; eval:0.4700; lr:0.000500
epoch:3; metric:emoval; train:0.4205; eval:0.5103; lr:0.000500
epoch:4; metric:emoval; train:0.5005; eval:0.4867; lr:0.000500
epoch:5; metric:emoval; train:0.5312; eval:0.5491; lr:0.000500
epoch:6; metric:emoval; train:0.5603; eval:0.5468; lr:0.000500
epoch:7; metric:emoval; train:0.6161; eval:0.5107; lr:0.000500
epoch:8; metric:emoval; train:0.6546; eval:0.5441; lr:0.000500
epoch:9; metric:emoval; train:0.6740; eval:0.5034; lr:0.000500
epoch:10; metric:emoval; train:0.6729; eval:0.5323; lr:0.000500
epoch:11; metric:emoval; train:0.6779; eval:0.5304; lr:0.000500
epoch:12; metric:emoval; train:0.7031; eval:0.6077; lr:0.000500
epoch:13; metric:emoval; train:0.7186; eval:0.5676; lr:0.000500
epoch:14; metric:emoval; train:0.7297; eval:0.5793; lr:0.000500
epoch:15; metric:emoval; train:0.7261; eval:0.5517; lr:0.000500
epoch:16; metric:emoval; train:0.7416; eval:0.5665; lr:0.000500
epoch:17; metric:emoval; train:0.7435; eval:0.5634; lr:0.000500
epoch:18; metric:emoval; train:0.7477; eval:0.5742; lr:0.000500
epoch:19; metric:emoval; train:0.7250; eval:0.5796; lr:0.000500
epoch:20; metric:emoval; train:0.7479; eval:0.5872; lr:0.000500
epoch:21; metric:emoval; train:0.7704; eval:0.5916; lr:0.000500
epoch:22; metric:emoval; train:0.7496; eval:0.5280; lr:0.000500
epoch:23; metric:emoval; train:0.7583; eval:0.5769; lr:0.000250
epoch:24; metric:emoval; train:0.7994; eval:0.6014; lr:0.000250
epoch:25; metric:emoval; train:0.8093; eval:0.5936; lr:0.000250
epoch:26; metric:emoval; train:0.8121; eval:0.6277; lr:0.000250
epoch:27; metric:emoval; train:0.8170; eval:0.6073; lr:0.000250
epoch:28; metric:emoval; train:0.8136; eval:0.6175; lr:0.000250
epoch:29; metric:emoval; train:0.7996; eval:0.5939; lr:0.000250
epoch:30; metric:emoval; train:0.8089; eval:0.5833; lr:0.000250
epoch:31; metric:emoval; train:0.8010; eval:0.6143; lr:0.000250
epoch:32; metric:emoval; train:0.8113; eval:0.5742; lr:0.000250
epoch:33; metric:emoval; train:0.8130; eval:0.5820; lr:0.000250
epoch:34; metric:emoval; train:0.7946; eval:0.6138; lr:0.000250
epoch:35; metric:emoval; train:0.8018; eval:0.5912; lr:0.000250
epoch:36; metric:emoval; train:0.8014; eval:0.5824; lr:0.000250
epoch:37; metric:emoval; train:0.8065; eval:0.6159; lr:0.000125
epoch:38; metric:emoval; train:0.8121; eval:0.6261; lr:0.000125
epoch:39; metric:emoval; train:0.8372; eval:0.6472; lr:0.000125
epoch:40; metric:emoval; train:0.8302; eval:0.6111; lr:0.000125
epoch:41; metric:emoval; train:0.8062; eval:0.6050; lr:0.000125
epoch:42; metric:emoval; train:0.8305; eval:0.6287; lr:0.000125
epoch:43; metric:emoval; train:0.8148; eval:0.6427; lr:0.000125
epoch:44; metric:emoval; train:0.8285; eval:0.6173; lr:0.000125
epoch:45; metric:emoval; train:0.8339; eval:0.6389; lr:0.000125
epoch:46; metric:emoval; train:0.8283; eval:0.6069; lr:0.000125
epoch:47; metric:emoval; train:0.8202; eval:0.6254; lr:0.000125
epoch:48; metric:emoval; train:0.8280; eval:0.6167; lr:0.000125
epoch:49; metric:emoval; train:0.8396; eval:0.6500; lr:0.000125
epoch:50; metric:emoval; train:0.8364; eval:0.6195; lr:0.000125
epoch:51; metric:emoval; train:0.8319; eval:0.6198; lr:0.000125
epoch:52; metric:emoval; train:0.8269; eval:0.6220; lr:0.000125
epoch:53; metric:emoval; train:0.8255; eval:0.6081; lr:0.000125
epoch:54; metric:emoval; train:0.8433; eval:0.6022; lr:0.000125
epoch:55; metric:emoval; train:0.8265; eval:0.6165; lr:0.000125
epoch:56; metric:emoval; train:0.8255; eval:0.6179; lr:0.000125
epoch:57; metric:emoval; train:0.8322; eval:0.6161; lr:0.000125
epoch:58; metric:emoval; train:0.8284; eval:0.6070; lr:0.000125
epoch:59; metric:emoval; train:0.8444; eval:0.5777; lr:0.000125
epoch:60; metric:emoval; train:0.8381; eval:0.5925; lr:0.000063
epoch:61; metric:emoval; train:0.8491; eval:0.6025; lr:0.000063
epoch:62; metric:emoval; train:0.8584; eval:0.5962; lr:0.000063
epoch:63; metric:emoval; train:0.8649; eval:0.6329; lr:0.000063
epoch:64; metric:emoval; train:0.8778; eval:0.6187; lr:0.000063
epoch:65; metric:emoval; train:0.8590; eval:0.6140; lr:0.000063
epoch:66; metric:emoval; train:0.8540; eval:0.6268; lr:0.000063
epoch:67; metric:emoval; train:0.8462; eval:0.6290; lr:0.000063
epoch:68; metric:emoval; train:0.8589; eval:0.6287; lr:0.000063
epoch:69; metric:emoval; train:0.8585; eval:0.6067; lr:0.000063
epoch:70; metric:emoval; train:0.8683; eval:0.6129; lr:0.000063
epoch:71; metric:emoval; train:0.8572; eval:0.6285; lr:0.000031
epoch:72; metric:emoval; train:0.8683; eval:0.6356; lr:0.000031
epoch:73; metric:emoval; train:0.8764; eval:0.6364; lr:0.000031
epoch:74; metric:emoval; train:0.8743; eval:0.6250; lr:0.000031
epoch:75; metric:emoval; train:0.8675; eval:0.6345; lr:0.000031
epoch:76; metric:emoval; train:0.8850; eval:0.6326; lr:0.000031
epoch:77; metric:emoval; train:0.8805; eval:0.6337; lr:0.000031
epoch:78; metric:emoval; train:0.8742; eval:0.6278; lr:0.000031
epoch:79; metric:emoval; train:0.8861; eval:0.6249; lr:0.000031
Early stopping at epoch 79, best epoch: 49
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 48, duration: 256.4543106555939 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1414; eval:0.2900; lr:0.000500
epoch:2; metric:emoval; train:0.2544; eval:0.4081; lr:0.000500
epoch:3; metric:emoval; train:0.3828; eval:0.4406; lr:0.000500
epoch:4; metric:emoval; train:0.4724; eval:0.5143; lr:0.000500
epoch:5; metric:emoval; train:0.5567; eval:0.4899; lr:0.000500
epoch:6; metric:emoval; train:0.5897; eval:0.5757; lr:0.000500
epoch:7; metric:emoval; train:0.6258; eval:0.5506; lr:0.000500
epoch:8; metric:emoval; train:0.6317; eval:0.5395; lr:0.000500
epoch:9; metric:emoval; train:0.6622; eval:0.5266; lr:0.000500
epoch:10; metric:emoval; train:0.7050; eval:0.5102; lr:0.000500
epoch:11; metric:emoval; train:0.6867; eval:0.5799; lr:0.000500
epoch:12; metric:emoval; train:0.6969; eval:0.5547; lr:0.000500
epoch:13; metric:emoval; train:0.7179; eval:0.5349; lr:0.000500
epoch:14; metric:emoval; train:0.7072; eval:0.5468; lr:0.000500
epoch:15; metric:emoval; train:0.7305; eval:0.4944; lr:0.000500
epoch:16; metric:emoval; train:0.7404; eval:0.5787; lr:0.000500
epoch:17; metric:emoval; train:0.7583; eval:0.5531; lr:0.000500
epoch:18; metric:emoval; train:0.7687; eval:0.5443; lr:0.000500
epoch:19; metric:emoval; train:0.7503; eval:0.5452; lr:0.000500
epoch:20; metric:emoval; train:0.7690; eval:0.5305; lr:0.000500
epoch:21; metric:emoval; train:0.7599; eval:0.5834; lr:0.000500
epoch:22; metric:emoval; train:0.7685; eval:0.5504; lr:0.000500
epoch:23; metric:emoval; train:0.7705; eval:0.5718; lr:0.000500
epoch:24; metric:emoval; train:0.7641; eval:0.5845; lr:0.000500
epoch:25; metric:emoval; train:0.7731; eval:0.5200; lr:0.000500
epoch:26; metric:emoval; train:0.7731; eval:0.5113; lr:0.000500
epoch:27; metric:emoval; train:0.7501; eval:0.4974; lr:0.000500
epoch:28; metric:emoval; train:0.7642; eval:0.5643; lr:0.000500
epoch:29; metric:emoval; train:0.7648; eval:0.4924; lr:0.000500
epoch:30; metric:emoval; train:0.7511; eval:0.5249; lr:0.000500
epoch:31; metric:emoval; train:0.7430; eval:0.5569; lr:0.000500
epoch:32; metric:emoval; train:0.7386; eval:0.5746; lr:0.000500
epoch:33; metric:emoval; train:0.7530; eval:0.5237; lr:0.000500
epoch:34; metric:emoval; train:0.7477; eval:0.5161; lr:0.000500
epoch:35; metric:emoval; train:0.7416; eval:0.5303; lr:0.000250
epoch:36; metric:emoval; train:0.7896; eval:0.5814; lr:0.000250
epoch:37; metric:emoval; train:0.7946; eval:0.5796; lr:0.000250
epoch:38; metric:emoval; train:0.7902; eval:0.5856; lr:0.000250
epoch:39; metric:emoval; train:0.8022; eval:0.5695; lr:0.000250
epoch:40; metric:emoval; train:0.8009; eval:0.5787; lr:0.000250
epoch:41; metric:emoval; train:0.7912; eval:0.5674; lr:0.000250
epoch:42; metric:emoval; train:0.7937; eval:0.5548; lr:0.000250
epoch:43; metric:emoval; train:0.8150; eval:0.5856; lr:0.000250
epoch:44; metric:emoval; train:0.7920; eval:0.5775; lr:0.000250
epoch:45; metric:emoval; train:0.7949; eval:0.5939; lr:0.000250
epoch:46; metric:emoval; train:0.7935; eval:0.5701; lr:0.000250
epoch:47; metric:emoval; train:0.7984; eval:0.5470; lr:0.000250
epoch:48; metric:emoval; train:0.7998; eval:0.5370; lr:0.000250
epoch:49; metric:emoval; train:0.8023; eval:0.5535; lr:0.000250
epoch:50; metric:emoval; train:0.8054; eval:0.6031; lr:0.000250
epoch:51; metric:emoval; train:0.8015; eval:0.5401; lr:0.000250
epoch:52; metric:emoval; train:0.8070; eval:0.5737; lr:0.000250
epoch:53; metric:emoval; train:0.8045; eval:0.5846; lr:0.000250
epoch:54; metric:emoval; train:0.8044; eval:0.5447; lr:0.000250
epoch:55; metric:emoval; train:0.8026; eval:0.5228; lr:0.000250
epoch:56; metric:emoval; train:0.7992; eval:0.5640; lr:0.000250
epoch:57; metric:emoval; train:0.8008; eval:0.5864; lr:0.000250
epoch:58; metric:emoval; train:0.7978; eval:0.5897; lr:0.000250
