====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v6', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='/root/autodl-tmp/MERTools-master/MERBench/attention_robust_v6/outputs/results-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 36%|███▌      | 1209/3373 [00:00<00:00, 12069.47it/s] 72%|███████▏  | 2433/3373 [00:00<00:00, 12126.84it/s]100%|██████████| 3373/3373 [00:00<00:00, 12012.30it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 30%|██▉       | 998/3373 [00:00<00:00, 9960.30it/s] 59%|█████▉    | 1995/3373 [00:00<00:00, 8684.25it/s] 86%|████████▌ | 2892/3373 [00:00<00:00, 8790.60it/s]100%|██████████| 3373/3373 [00:00<00:00, 8710.67it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 47%|████▋     | 1587/3373 [00:00<00:00, 15859.31it/s] 94%|█████████▍| 3173/3373 [00:00<00:00, 14580.70it/s]100%|██████████| 3373/3373 [00:00<00:00, 14952.95it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 8663.30it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 11657.54it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 15526.62it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 14010.94it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 10888.74it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 19178.43it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 15134.10it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 11467.21it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 15858.34it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3949; eval:-0.0625; lr:0.000500
epoch:2; metric:emoval; train:0.1027; eval:0.3150; lr:0.000500
epoch:3; metric:emoval; train:0.3431; eval:0.5475; lr:0.000500
epoch:4; metric:emoval; train:0.4956; eval:0.5503; lr:0.000500
epoch:5; metric:emoval; train:0.5613; eval:0.4557; lr:0.000500
epoch:6; metric:emoval; train:0.6216; eval:0.5366; lr:0.000500
epoch:7; metric:emoval; train:0.6433; eval:0.5400; lr:0.000500
epoch:8; metric:emoval; train:0.6547; eval:0.5852; lr:0.000500
epoch:9; metric:emoval; train:0.6801; eval:0.4822; lr:0.000500
epoch:10; metric:emoval; train:0.6889; eval:0.5344; lr:0.000500
epoch:11; metric:emoval; train:0.6948; eval:0.4843; lr:0.000500
epoch:12; metric:emoval; train:0.7050; eval:0.5861; lr:0.000500
epoch:13; metric:emoval; train:0.7189; eval:0.5642; lr:0.000500
epoch:14; metric:emoval; train:0.7338; eval:0.5379; lr:0.000500
epoch:15; metric:emoval; train:0.7372; eval:0.5505; lr:0.000500
epoch:16; metric:emoval; train:0.7266; eval:0.5644; lr:0.000500
epoch:17; metric:emoval; train:0.7483; eval:0.5402; lr:0.000500
epoch:18; metric:emoval; train:0.7529; eval:0.5307; lr:0.000500
epoch:19; metric:emoval; train:0.7627; eval:0.5770; lr:0.000500
epoch:20; metric:emoval; train:0.7720; eval:0.4553; lr:0.000500
epoch:21; metric:emoval; train:0.7624; eval:0.5207; lr:0.000500
epoch:22; metric:emoval; train:0.7562; eval:0.5627; lr:0.000500
epoch:23; metric:emoval; train:0.7524; eval:0.5818; lr:0.000250
epoch:24; metric:emoval; train:0.7994; eval:0.6103; lr:0.000250
epoch:25; metric:emoval; train:0.8221; eval:0.6109; lr:0.000250
epoch:26; metric:emoval; train:0.8136; eval:0.5855; lr:0.000250
epoch:27; metric:emoval; train:0.8149; eval:0.6086; lr:0.000250
epoch:28; metric:emoval; train:0.8086; eval:0.5973; lr:0.000250
epoch:29; metric:emoval; train:0.7950; eval:0.5843; lr:0.000250
epoch:30; metric:emoval; train:0.8171; eval:0.5806; lr:0.000250
epoch:31; metric:emoval; train:0.7945; eval:0.5911; lr:0.000250
epoch:32; metric:emoval; train:0.8087; eval:0.5886; lr:0.000250
epoch:33; metric:emoval; train:0.8157; eval:0.5827; lr:0.000250
epoch:34; metric:emoval; train:0.8197; eval:0.5643; lr:0.000250
epoch:35; metric:emoval; train:0.8027; eval:0.5781; lr:0.000250
epoch:36; metric:emoval; train:0.8067; eval:0.5904; lr:0.000125
epoch:37; metric:emoval; train:0.8308; eval:0.5900; lr:0.000125
epoch:38; metric:emoval; train:0.8157; eval:0.5955; lr:0.000125
epoch:39; metric:emoval; train:0.8328; eval:0.5951; lr:0.000125
epoch:40; metric:emoval; train:0.8361; eval:0.5761; lr:0.000125
epoch:41; metric:emoval; train:0.8269; eval:0.6002; lr:0.000125
epoch:42; metric:emoval; train:0.8236; eval:0.6020; lr:0.000125
epoch:43; metric:emoval; train:0.8133; eval:0.5726; lr:0.000125
epoch:44; metric:emoval; train:0.8279; eval:0.5794; lr:0.000125
epoch:45; metric:emoval; train:0.8194; eval:0.5978; lr:0.000125
epoch:46; metric:emoval; train:0.8387; eval:0.5852; lr:0.000125
epoch:47; metric:emoval; train:0.8289; eval:0.6130; lr:0.000125
epoch:48; metric:emoval; train:0.8373; eval:0.5727; lr:0.000125
epoch:49; metric:emoval; train:0.8406; eval:0.5891; lr:0.000125
epoch:50; metric:emoval; train:0.8222; eval:0.5934; lr:0.000125
epoch:51; metric:emoval; train:0.8206; eval:0.5911; lr:0.000125
epoch:52; metric:emoval; train:0.8344; eval:0.5841; lr:0.000125
epoch:53; metric:emoval; train:0.8297; eval:0.5739; lr:0.000125
epoch:54; metric:emoval; train:0.8231; eval:0.5781; lr:0.000125
epoch:55; metric:emoval; train:0.8254; eval:0.5931; lr:0.000125
epoch:56; metric:emoval; train:0.8442; eval:0.5878; lr:0.000125
epoch:57; metric:emoval; train:0.8298; eval:0.5824; lr:0.000125
epoch:58; metric:emoval; train:0.8280; eval:0.5726; lr:0.000063
epoch:59; metric:emoval; train:0.8478; eval:0.5938; lr:0.000063
epoch:60; metric:emoval; train:0.8571; eval:0.6170; lr:0.000063
epoch:61; metric:emoval; train:0.8668; eval:0.6093; lr:0.000063
epoch:62; metric:emoval; train:0.8423; eval:0.5907; lr:0.000063
epoch:63; metric:emoval; train:0.8595; eval:0.5869; lr:0.000063
epoch:64; metric:emoval; train:0.8618; eval:0.5897; lr:0.000063
epoch:65; metric:emoval; train:0.8659; eval:0.6072; lr:0.000063
epoch:66; metric:emoval; train:0.8540; eval:0.5976; lr:0.000063
epoch:67; metric:emoval; train:0.8578; eval:0.6195; lr:0.000063
epoch:68; metric:emoval; train:0.8577; eval:0.6186; lr:0.000063
epoch:69; metric:emoval; train:0.8701; eval:0.6016; lr:0.000063
epoch:70; metric:emoval; train:0.8567; eval:0.6094; lr:0.000063
epoch:71; metric:emoval; train:0.8599; eval:0.5987; lr:0.000063
epoch:72; metric:emoval; train:0.8576; eval:0.5924; lr:0.000063
epoch:73; metric:emoval; train:0.8608; eval:0.6031; lr:0.000063
epoch:74; metric:emoval; train:0.8623; eval:0.6181; lr:0.000063
epoch:75; metric:emoval; train:0.8674; eval:0.5843; lr:0.000063
epoch:76; metric:emoval; train:0.8674; eval:0.6158; lr:0.000063
epoch:77; metric:emoval; train:0.8684; eval:0.6142; lr:0.000063
epoch:78; metric:emoval; train:0.8530; eval:0.5987; lr:0.000031
epoch:79; metric:emoval; train:0.8597; eval:0.6090; lr:0.000031
epoch:80; metric:emoval; train:0.8745; eval:0.5940; lr:0.000031
epoch:81; metric:emoval; train:0.8741; eval:0.6105; lr:0.000031
epoch:82; metric:emoval; train:0.8701; eval:0.6076; lr:0.000031
epoch:83; metric:emoval; train:0.8676; eval:0.5945; lr:0.000031
epoch:84; metric:emoval; train:0.8548; eval:0.6028; lr:0.000031
epoch:85; metric:emoval; train:0.8806; eval:0.6074; lr:0.000031
epoch:86; metric:emoval; train:0.8758; eval:0.6067; lr:0.000031
epoch:87; metric:emoval; train:0.8831; eval:0.5850; lr:0.000031
epoch:88; metric:emoval; train:0.8633; eval:0.6141; lr:0.000031
epoch:89; metric:emoval; train:0.8811; eval:0.5958; lr:0.000016
epoch:90; metric:emoval; train:0.8882; eval:0.6167; lr:0.000016
epoch:91; metric:emoval; train:0.8844; eval:0.6231; lr:0.000016
epoch:92; metric:emoval; train:0.8770; eval:0.6012; lr:0.000016
epoch:93; metric:emoval; train:0.8756; eval:0.6043; lr:0.000016
epoch:94; metric:emoval; train:0.8898; eval:0.5950; lr:0.000016
epoch:95; metric:emoval; train:0.8785; eval:0.6067; lr:0.000016
epoch:96; metric:emoval; train:0.8858; eval:0.6161; lr:0.000016
epoch:97; metric:emoval; train:0.8831; eval:0.6058; lr:0.000016
epoch:98; metric:emoval; train:0.8767; eval:0.6120; lr:0.000016
epoch:99; metric:emoval; train:0.8759; eval:0.6008; lr:0.000016
epoch:100; metric:emoval; train:0.8759; eval:0.6059; lr:0.000016
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 90, duration: 282.6717803478241 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4936; eval:0.0640; lr:0.000500
epoch:2; metric:emoval; train:0.0814; eval:0.2719; lr:0.000500
epoch:3; metric:emoval; train:0.2984; eval:0.2469; lr:0.000500
epoch:4; metric:emoval; train:0.4815; eval:0.5266; lr:0.000500
epoch:5; metric:emoval; train:0.5539; eval:0.4386; lr:0.000500
epoch:6; metric:emoval; train:0.5869; eval:0.5224; lr:0.000500
epoch:7; metric:emoval; train:0.6354; eval:0.5621; lr:0.000500
epoch:8; metric:emoval; train:0.6569; eval:0.5127; lr:0.000500
epoch:9; metric:emoval; train:0.6684; eval:0.5410; lr:0.000500
epoch:10; metric:emoval; train:0.6810; eval:0.5486; lr:0.000500
epoch:11; metric:emoval; train:0.6917; eval:0.4958; lr:0.000500
epoch:12; metric:emoval; train:0.7052; eval:0.4765; lr:0.000500
epoch:13; metric:emoval; train:0.7089; eval:0.5708; lr:0.000500
epoch:14; metric:emoval; train:0.7261; eval:0.5217; lr:0.000500
epoch:15; metric:emoval; train:0.7347; eval:0.5655; lr:0.000500
epoch:16; metric:emoval; train:0.7556; eval:0.5191; lr:0.000500
epoch:17; metric:emoval; train:0.7495; eval:0.5556; lr:0.000500
epoch:18; metric:emoval; train:0.7516; eval:0.5604; lr:0.000500
epoch:19; metric:emoval; train:0.7744; eval:0.4983; lr:0.000500
epoch:20; metric:emoval; train:0.7631; eval:0.5116; lr:0.000500
epoch:21; metric:emoval; train:0.7538; eval:0.5529; lr:0.000500
epoch:22; metric:emoval; train:0.7554; eval:0.5338; lr:0.000500
epoch:23; metric:emoval; train:0.7516; eval:0.5244; lr:0.000500
epoch:24; metric:emoval; train:0.7612; eval:0.5231; lr:0.000250
epoch:25; metric:emoval; train:0.7981; eval:0.5587; lr:0.000250
epoch:26; metric:emoval; train:0.8246; eval:0.5660; lr:0.000250
epoch:27; metric:emoval; train:0.8188; eval:0.5778; lr:0.000250
epoch:28; metric:emoval; train:0.8164; eval:0.5462; lr:0.000250
epoch:29; metric:emoval; train:0.8225; eval:0.5512; lr:0.000250
epoch:30; metric:emoval; train:0.7926; eval:0.4952; lr:0.000250
epoch:31; metric:emoval; train:0.8090; eval:0.5345; lr:0.000250
epoch:32; metric:emoval; train:0.8171; eval:0.5357; lr:0.000250
epoch:33; metric:emoval; train:0.7942; eval:0.5346; lr:0.000250
epoch:34; metric:emoval; train:0.8029; eval:0.5500; lr:0.000250
epoch:35; metric:emoval; train:0.7968; eval:0.5558; lr:0.000250
epoch:36; metric:emoval; train:0.7920; eval:0.5389; lr:0.000250
epoch:37; metric:emoval; train:0.7992; eval:0.5443; lr:0.000250
epoch:38; metric:emoval; train:0.7882; eval:0.5746; lr:0.000125
epoch:39; metric:emoval; train:0.8137; eval:0.6004; lr:0.000125
epoch:40; metric:emoval; train:0.8210; eval:0.5894; lr:0.000125
epoch:41; metric:emoval; train:0.7984; eval:0.5762; lr:0.000125
epoch:42; metric:emoval; train:0.8233; eval:0.5798; lr:0.000125
epoch:43; metric:emoval; train:0.8189; eval:0.5768; lr:0.000125
epoch:44; metric:emoval; train:0.8288; eval:0.5499; lr:0.000125
epoch:45; metric:emoval; train:0.8414; eval:0.5760; lr:0.000125
epoch:46; metric:emoval; train:0.8322; eval:0.5705; lr:0.000125
epoch:47; metric:emoval; train:0.8211; eval:0.5629; lr:0.000125
epoch:48; metric:emoval; train:0.8307; eval:0.5790; lr:0.000125
epoch:49; metric:emoval; train:0.8186; eval:0.5775; lr:0.000125
epoch:50; metric:emoval; train:0.8245; eval:0.5814; lr:0.000063
epoch:51; metric:emoval; train:0.8539; eval:0.5942; lr:0.000063
epoch:52; metric:emoval; train:0.8357; eval:0.5959; lr:0.000063
epoch:53; metric:emoval; train:0.8605; eval:0.6070; lr:0.000063
epoch:54; metric:emoval; train:0.8578; eval:0.5746; lr:0.000063
epoch:55; metric:emoval; train:0.8587; eval:0.5889; lr:0.000063
epoch:56; metric:emoval; train:0.8557; eval:0.5742; lr:0.000063
epoch:57; metric:emoval; train:0.8643; eval:0.5746; lr:0.000063
epoch:58; metric:emoval; train:0.8706; eval:0.6004; lr:0.000063
epoch:59; metric:emoval; train:0.8549; eval:0.5756; lr:0.000063
epoch:60; metric:emoval; train:0.8480; eval:0.5949; lr:0.000063
epoch:61; metric:emoval; train:0.8414; eval:0.5885; lr:0.000063
epoch:62; metric:emoval; train:0.8497; eval:0.5818; lr:0.000063
epoch:63; metric:emoval; train:0.8583; eval:0.5818; lr:0.000063
epoch:64; metric:emoval; train:0.8419; eval:0.5700; lr:0.000031
epoch:65; metric:emoval; train:0.8499; eval:0.5905; lr:0.000031
epoch:66; metric:emoval; train:0.8602; eval:0.5851; lr:0.000031
epoch:67; metric:emoval; train:0.8655; eval:0.5695; lr:0.000031
epoch:68; metric:emoval; train:0.8640; eval:0.5841; lr:0.000031
epoch:69; metric:emoval; train:0.8617; eval:0.5783; lr:0.000031
epoch:70; metric:emoval; train:0.8721; eval:0.5881; lr:0.000031
epoch:71; metric:emoval; train:0.8681; eval:0.5997; lr:0.000031
epoch:72; metric:emoval; train:0.8625; eval:0.5953; lr:0.000031
epoch:73; metric:emoval; train:0.8788; eval:0.5790; lr:0.000031
epoch:74; metric:emoval; train:0.8713; eval:0.5794; lr:0.000031
epoch:75; metric:emoval; train:0.8719; eval:0.5896; lr:0.000016
epoch:76; metric:emoval; train:0.8801; eval:0.6034; lr:0.000016
epoch:77; metric:emoval; train:0.8653; eval:0.5917; lr:0.000016
epoch:78; metric:emoval; train:0.8774; eval:0.5854; lr:0.000016
epoch:79; metric:emoval; train:0.8743; eval:0.5926; lr:0.000016
epoch:80; metric:emoval; train:0.8762; eval:0.5886; lr:0.000016
epoch:81; metric:emoval; train:0.8790; eval:0.5932; lr:0.000016
epoch:82; metric:emoval; train:0.8711; eval:0.5862; lr:0.000016
epoch:83; metric:emoval; train:0.8771; eval:0.5930; lr:0.000016
Early stopping at epoch 83, best epoch: 53
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 52, duration: 241.8096387386322 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3233; eval:0.1576; lr:0.000500
epoch:2; metric:emoval; train:0.2133; eval:0.4124; lr:0.000500
epoch:3; metric:emoval; train:0.4303; eval:0.5506; lr:0.000500
epoch:4; metric:emoval; train:0.5432; eval:0.5294; lr:0.000500
epoch:5; metric:emoval; train:0.5890; eval:0.5796; lr:0.000500
epoch:6; metric:emoval; train:0.6010; eval:0.5512; lr:0.000500
epoch:7; metric:emoval; train:0.6531; eval:0.5426; lr:0.000500
epoch:8; metric:emoval; train:0.6371; eval:0.5787; lr:0.000500
epoch:9; metric:emoval; train:0.6722; eval:0.5360; lr:0.000500
epoch:10; metric:emoval; train:0.6989; eval:0.5482; lr:0.000500
epoch:11; metric:emoval; train:0.7141; eval:0.6215; lr:0.000500
epoch:12; metric:emoval; train:0.7167; eval:0.4210; lr:0.000500
epoch:13; metric:emoval; train:0.7243; eval:0.5695; lr:0.000500
epoch:14; metric:emoval; train:0.7295; eval:0.4821; lr:0.000500
epoch:15; metric:emoval; train:0.7197; eval:0.5460; lr:0.000500
epoch:16; metric:emoval; train:0.7270; eval:0.5630; lr:0.000500
epoch:17; metric:emoval; train:0.7475; eval:0.5866; lr:0.000500
epoch:18; metric:emoval; train:0.7389; eval:0.5442; lr:0.000500
epoch:19; metric:emoval; train:0.7470; eval:0.5952; lr:0.000500
epoch:20; metric:emoval; train:0.7649; eval:0.5539; lr:0.000500
epoch:21; metric:emoval; train:0.7483; eval:0.5568; lr:0.000500
epoch:22; metric:emoval; train:0.7593; eval:0.5898; lr:0.000250
epoch:23; metric:emoval; train:0.8098; eval:0.6055; lr:0.000250
epoch:24; metric:emoval; train:0.8139; eval:0.5991; lr:0.000250
epoch:25; metric:emoval; train:0.8115; eval:0.5929; lr:0.000250
epoch:26; metric:emoval; train:0.8168; eval:0.5736; lr:0.000250
epoch:27; metric:emoval; train:0.8140; eval:0.5781; lr:0.000250
epoch:28; metric:emoval; train:0.8033; eval:0.5332; lr:0.000250
epoch:29; metric:emoval; train:0.7950; eval:0.6011; lr:0.000250
epoch:30; metric:emoval; train:0.8072; eval:0.5699; lr:0.000250
epoch:31; metric:emoval; train:0.7938; eval:0.5606; lr:0.000250
epoch:32; metric:emoval; train:0.8144; eval:0.5218; lr:0.000250
epoch:33; metric:emoval; train:0.8001; eval:0.5898; lr:0.000125
epoch:34; metric:emoval; train:0.8215; eval:0.5899; lr:0.000125
epoch:35; metric:emoval; train:0.8456; eval:0.6149; lr:0.000125
epoch:36; metric:emoval; train:0.8283; eval:0.6232; lr:0.000125
epoch:37; metric:emoval; train:0.8297; eval:0.6310; lr:0.000125
epoch:38; metric:emoval; train:0.8402; eval:0.5986; lr:0.000125
epoch:39; metric:emoval; train:0.8300; eval:0.6171; lr:0.000125
epoch:40; metric:emoval; train:0.8301; eval:0.6111; lr:0.000125
epoch:41; metric:emoval; train:0.8064; eval:0.5927; lr:0.000125
epoch:42; metric:emoval; train:0.8332; eval:0.6229; lr:0.000125
epoch:43; metric:emoval; train:0.8200; eval:0.6132; lr:0.000125
epoch:44; metric:emoval; train:0.8150; eval:0.6174; lr:0.000125
epoch:45; metric:emoval; train:0.8205; eval:0.5798; lr:0.000125
epoch:46; metric:emoval; train:0.8321; eval:0.5988; lr:0.000125
epoch:47; metric:emoval; train:0.8236; eval:0.5879; lr:0.000125
epoch:48; metric:emoval; train:0.8309; eval:0.6139; lr:0.000063
epoch:49; metric:emoval; train:0.8456; eval:0.6026; lr:0.000063
epoch:50; metric:emoval; train:0.8555; eval:0.6135; lr:0.000063
epoch:51; metric:emoval; train:0.8518; eval:0.5978; lr:0.000063
epoch:52; metric:emoval; train:0.8469; eval:0.6059; lr:0.000063
epoch:53; metric:emoval; train:0.8494; eval:0.6101; lr:0.000063
epoch:54; metric:emoval; train:0.8562; eval:0.6233; lr:0.000063
epoch:55; metric:emoval; train:0.8524; eval:0.6056; lr:0.000063
epoch:56; metric:emoval; train:0.8501; eval:0.6036; lr:0.000063
epoch:57; metric:emoval; train:0.8579; eval:0.6187; lr:0.000063
epoch:58; metric:emoval; train:0.8332; eval:0.6096; lr:0.000063
epoch:59; metric:emoval; train:0.8512; eval:0.6017; lr:0.000031
epoch:60; metric:emoval; train:0.8582; eval:0.6243; lr:0.000031
epoch:61; metric:emoval; train:0.8770; eval:0.6009; lr:0.000031
epoch:62; metric:emoval; train:0.8590; eval:0.6369; lr:0.000031
epoch:63; metric:emoval; train:0.8567; eval:0.6256; lr:0.000031
epoch:64; metric:emoval; train:0.8632; eval:0.6316; lr:0.000031
epoch:65; metric:emoval; train:0.8614; eval:0.6254; lr:0.000031
epoch:66; metric:emoval; train:0.8604; eval:0.6204; lr:0.000031
epoch:67; metric:emoval; train:0.8568; eval:0.6128; lr:0.000031
epoch:68; metric:emoval; train:0.8672; eval:0.6098; lr:0.000031
epoch:69; metric:emoval; train:0.8554; eval:0.6254; lr:0.000031
epoch:70; metric:emoval; train:0.8628; eval:0.6319; lr:0.000031
epoch:71; metric:emoval; train:0.8597; eval:0.6183; lr:0.000031
epoch:72; metric:emoval; train:0.8734; eval:0.6246; lr:0.000031
epoch:73; metric:emoval; train:0.8677; eval:0.6155; lr:0.000016
epoch:74; metric:emoval; train:0.8693; eval:0.6227; lr:0.000016
epoch:75; metric:emoval; train:0.8646; eval:0.6232; lr:0.000016
epoch:76; metric:emoval; train:0.8711; eval:0.6236; lr:0.000016
epoch:77; metric:emoval; train:0.8652; eval:0.6252; lr:0.000016
epoch:78; metric:emoval; train:0.8748; eval:0.6206; lr:0.000016
epoch:79; metric:emoval; train:0.8614; eval:0.6083; lr:0.000016
epoch:80; metric:emoval; train:0.8634; eval:0.6199; lr:0.000016
epoch:81; metric:emoval; train:0.8662; eval:0.6160; lr:0.000016
epoch:82; metric:emoval; train:0.8785; eval:0.6202; lr:0.000016
epoch:83; metric:emoval; train:0.8730; eval:0.6229; lr:0.000016
epoch:84; metric:emoval; train:0.8695; eval:0.6255; lr:0.000008
epoch:85; metric:emoval; train:0.8789; eval:0.6229; lr:0.000008
epoch:86; metric:emoval; train:0.8774; eval:0.6228; lr:0.000008
epoch:87; metric:emoval; train:0.8829; eval:0.6310; lr:0.000008
epoch:88; metric:emoval; train:0.8844; eval:0.6263; lr:0.000008
epoch:89; metric:emoval; train:0.8775; eval:0.6133; lr:0.000008
epoch:90; metric:emoval; train:0.8718; eval:0.6264; lr:0.000008
epoch:91; metric:emoval; train:0.8864; eval:0.6240; lr:0.000008
epoch:92; metric:emoval; train:0.8819; eval:0.6137; lr:0.000008
Early stopping at epoch 92, best epoch: 62
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 61, duration: 259.02527594566345 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4347; eval:0.0756; lr:0.000500
epoch:2; metric:emoval; train:0.0781; eval:0.2012; lr:0.000500
epoch:3; metric:emoval; train:0.3110; eval:0.4652; lr:0.000500
epoch:4; metric:emoval; train:0.4557; eval:0.5263; lr:0.000500
epoch:5; metric:emoval; train:0.5331; eval:0.5126; lr:0.000500
epoch:6; metric:emoval; train:0.5828; eval:0.5140; lr:0.000500
epoch:7; metric:emoval; train:0.6301; eval:0.5189; lr:0.000500
epoch:8; metric:emoval; train:0.6368; eval:0.5252; lr:0.000500
epoch:9; metric:emoval; train:0.6586; eval:0.5148; lr:0.000500
epoch:10; metric:emoval; train:0.6733; eval:0.5508; lr:0.000500
epoch:11; metric:emoval; train:0.7022; eval:0.5045; lr:0.000500
epoch:12; metric:emoval; train:0.7031; eval:0.5506; lr:0.000500
epoch:13; metric:emoval; train:0.7293; eval:0.5504; lr:0.000500
epoch:14; metric:emoval; train:0.7380; eval:0.5416; lr:0.000500
epoch:15; metric:emoval; train:0.7353; eval:0.5171; lr:0.000500
epoch:16; metric:emoval; train:0.7325; eval:0.4622; lr:0.000500
epoch:17; metric:emoval; train:0.7443; eval:0.5336; lr:0.000500
epoch:18; metric:emoval; train:0.7495; eval:0.4801; lr:0.000500
epoch:19; metric:emoval; train:0.7459; eval:0.5276; lr:0.000500
epoch:20; metric:emoval; train:0.7706; eval:0.5180; lr:0.000500
epoch:21; metric:emoval; train:0.7517; eval:0.5707; lr:0.000500
epoch:22; metric:emoval; train:0.7664; eval:0.5200; lr:0.000500
epoch:23; metric:emoval; train:0.7597; eval:0.5695; lr:0.000500
epoch:24; metric:emoval; train:0.7672; eval:0.5991; lr:0.000500
epoch:25; metric:emoval; train:0.7530; eval:0.5369; lr:0.000500
epoch:26; metric:emoval; train:0.7418; eval:0.5395; lr:0.000500
epoch:27; metric:emoval; train:0.7476; eval:0.5648; lr:0.000500
epoch:28; metric:emoval; train:0.7506; eval:0.6055; lr:0.000500
epoch:29; metric:emoval; train:0.7471; eval:0.5392; lr:0.000500
epoch:30; metric:emoval; train:0.7483; eval:0.5581; lr:0.000500
epoch:31; metric:emoval; train:0.7635; eval:0.4994; lr:0.000500
epoch:32; metric:emoval; train:0.7424; eval:0.6027; lr:0.000500
epoch:33; metric:emoval; train:0.7439; eval:0.5934; lr:0.000500
epoch:34; metric:emoval; train:0.7553; eval:0.5253; lr:0.000500
epoch:35; metric:emoval; train:0.7453; eval:0.5891; lr:0.000500
epoch:36; metric:emoval; train:0.7432; eval:0.5649; lr:0.000500
epoch:37; metric:emoval; train:0.7157; eval:0.5373; lr:0.000500
epoch:38; metric:emoval; train:0.7525; eval:0.5947; lr:0.000500
epoch:39; metric:emoval; train:0.7457; eval:0.5597; lr:0.000250
epoch:40; metric:emoval; train:0.7860; eval:0.6216; lr:0.000250
epoch:41; metric:emoval; train:0.7903; eval:0.5525; lr:0.000250
epoch:42; metric:emoval; train:0.7773; eval:0.5581; lr:0.000250
epoch:43; metric:emoval; train:0.7908; eval:0.5753; lr:0.000250
epoch:44; metric:emoval; train:0.7964; eval:0.5848; lr:0.000250
epoch:45; metric:emoval; train:0.7839; eval:0.5650; lr:0.000250
epoch:46; metric:emoval; train:0.7901; eval:0.5429; lr:0.000250
epoch:47; metric:emoval; train:0.7968; eval:0.5797; lr:0.000250
epoch:48; metric:emoval; train:0.7884; eval:0.6101; lr:0.000250
epoch:49; metric:emoval; train:0.7981; eval:0.5965; lr:0.000250
epoch:50; metric:emoval; train:0.8019; eval:0.5209; lr:0.000250
epoch:51; metric:emoval; train:0.8096; eval:0.6083; lr:0.000125
epoch:52; metric:emoval; train:0.8333; eval:0.6056; lr:0.000125
epoch:53; metric:emoval; train:0.8278; eval:0.6101; lr:0.000125
epoch:54; metric:emoval; train:0.8305; eval:0.6051; lr:0.000125
epoch:55; metric:emoval; train:0.8362; eval:0.6202; lr:0.000125
epoch:56; metric:emoval; train:0.8351; eval:0.6146; lr:0.000125
epoch:57; metric:emoval; train:0.8398; eval:0.6263; lr:0.000125
epoch:58; metric:emoval; train:0.8411; eval:0.6035; lr:0.000125
epoch:59; metric:emoval; train:0.8417; eval:0.6141; lr:0.000125
epoch:60; metric:emoval; train:0.8462; eval:0.6060; lr:0.000125
epoch:61; metric:emoval; train:0.8390; eval:0.5792; lr:0.000125
epoch:62; metric:emoval; train:0.8465; eval:0.5830; lr:0.000125
epoch:63; metric:emoval; train:0.8451; eval:0.5712; lr:0.000125
epoch:64; metric:emoval; train:0.8507; eval:0.5805; lr:0.000125
epoch:65; metric:emoval; train:0.8349; eval:0.5777; lr:0.000125
epoch:66; metric:emoval; train:0.8412; eval:0.6071; lr:0.000125
epoch:67; metric:emoval; train:0.8350; eval:0.6219; lr:0.000125
epoch:68; metric:emoval; train:0.8427; eval:0.5820; lr:0.000063
epoch:69; metric:emoval; train:0.8667; eval:0.6091; lr:0.000063
epoch:70; metric:emoval; train:0.8491; eval:0.5987; lr:0.000063
epoch:71; metric:emoval; train:0.8536; eval:0.6060; lr:0.000063
epoch:72; metric:emoval; train:0.8526; eval:0.6245; lr:0.000063
epoch:73; metric:emoval; train:0.8674; eval:0.6143; lr:0.000063
epoch:74; metric:emoval; train:0.8679; eval:0.6246; lr:0.000063
epoch:75; metric:emoval; train:0.8581; eval:0.6254; lr:0.000063
epoch:76; metric:emoval; train:0.8502; eval:0.6100; lr:0.000063
epoch:77; metric:emoval; train:0.8687; eval:0.6314; lr:0.000063
epoch:78; metric:emoval; train:0.8660; eval:0.6215; lr:0.000063
epoch:79; metric:emoval; train:0.8643; eval:0.6081; lr:0.000063
epoch:80; metric:emoval; train:0.8720; eval:0.6061; lr:0.000063
epoch:81; metric:emoval; train:0.8619; eval:0.5989; lr:0.000063
epoch:82; metric:emoval; train:0.8585; eval:0.6144; lr:0.000063
epoch:83; metric:emoval; train:0.8697; eval:0.6233; lr:0.000063
epoch:84; metric:emoval; train:0.8709; eval:0.6332; lr:0.000063
epoch:85; metric:emoval; train:0.8529; eval:0.6077; lr:0.000063
epoch:86; metric:emoval; train:0.8713; eval:0.5925; lr:0.000063
epoch:87; metric:emoval; train:0.8511; eval:0.5923; lr:0.000063
epoch:88; metric:emoval; train:0.8664; eval:0.6066; lr:0.000063
epoch:89; metric:emoval; train:0.8655; eval:0.5922; lr:0.000063
epoch:90; metric:emoval; train:0.8718; eval:0.6072; lr:0.000063
epoch:91; metric:emoval; train:0.8697; eval:0.6046; lr:0.000063
epoch:92; metric:emoval; train:0.8707; eval:0.6008; lr:0.000063
epoch:93; metric:emoval; train:0.8720; eval:0.6240; lr:0.000063
epoch:94; metric:emoval; train:0.8650; eval:0.6169; lr:0.000063
epoch:95; metric:emoval; train:0.8733; eval:0.6009; lr:0.000031
epoch:96; metric:emoval; train:0.8740; eval:0.6223; lr:0.000031
epoch:97; metric:emoval; train:0.8801; eval:0.6169; lr:0.000031
epoch:98; metric:emoval; train:0.8721; eval:0.6138; lr:0.000031
epoch:99; metric:emoval; train:0.8716; eval:0.6152; lr:0.000031
epoch:100; metric:emoval; train:0.8766; eval:0.6172; lr:0.000031
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 83, duration: 288.06406712532043 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3553; eval:0.1148; lr:0.000500
epoch:2; metric:emoval; train:0.0953; eval:0.3691; lr:0.000500
epoch:3; metric:emoval; train:0.4083; eval:0.3896; lr:0.000500
epoch:4; metric:emoval; train:0.5126; eval:0.5025; lr:0.000500
epoch:5; metric:emoval; train:0.5556; eval:0.5611; lr:0.000500
epoch:6; metric:emoval; train:0.6245; eval:0.5576; lr:0.000500
epoch:7; metric:emoval; train:0.6357; eval:0.5038; lr:0.000500
epoch:8; metric:emoval; train:0.6393; eval:0.5429; lr:0.000500
epoch:9; metric:emoval; train:0.6501; eval:0.5662; lr:0.000500
epoch:10; metric:emoval; train:0.6559; eval:0.5525; lr:0.000500
epoch:11; metric:emoval; train:0.6377; eval:0.5602; lr:0.000500
epoch:12; metric:emoval; train:0.6803; eval:0.5517; lr:0.000500
epoch:13; metric:emoval; train:0.6768; eval:0.5336; lr:0.000500
epoch:14; metric:emoval; train:0.6987; eval:0.5617; lr:0.000500
epoch:15; metric:emoval; train:0.7186; eval:0.5496; lr:0.000500
epoch:16; metric:emoval; train:0.7156; eval:0.4598; lr:0.000500
epoch:17; metric:emoval; train:0.7045; eval:0.5622; lr:0.000500
epoch:18; metric:emoval; train:0.7232; eval:0.5560; lr:0.000500
epoch:19; metric:emoval; train:0.7357; eval:0.5348; lr:0.000500
epoch:20; metric:emoval; train:0.7364; eval:0.5273; lr:0.000250
epoch:21; metric:emoval; train:0.7755; eval:0.5725; lr:0.000250
epoch:22; metric:emoval; train:0.7660; eval:0.5873; lr:0.000250
epoch:23; metric:emoval; train:0.7962; eval:0.5589; lr:0.000250
epoch:24; metric:emoval; train:0.7873; eval:0.5832; lr:0.000250
epoch:25; metric:emoval; train:0.7922; eval:0.5890; lr:0.000250
epoch:26; metric:emoval; train:0.7942; eval:0.5696; lr:0.000250
epoch:27; metric:emoval; train:0.7772; eval:0.5508; lr:0.000250
epoch:28; metric:emoval; train:0.7751; eval:0.5891; lr:0.000250
epoch:29; metric:emoval; train:0.7991; eval:0.5219; lr:0.000250
epoch:30; metric:emoval; train:0.7823; eval:0.5721; lr:0.000250
epoch:31; metric:emoval; train:0.7815; eval:0.5885; lr:0.000250
epoch:32; metric:emoval; train:0.7725; eval:0.5850; lr:0.000250
epoch:33; metric:emoval; train:0.7659; eval:0.5691; lr:0.000250
epoch:34; metric:emoval; train:0.7545; eval:0.5255; lr:0.000250
epoch:35; metric:emoval; train:0.7620; eval:0.5256; lr:0.000250
epoch:36; metric:emoval; train:0.7658; eval:0.5691; lr:0.000250
epoch:37; metric:emoval; train:0.7850; eval:0.5921; lr:0.000250
epoch:38; metric:emoval; train:0.7688; eval:0.5436; lr:0.000250
epoch:39; metric:emoval; train:0.7680; eval:0.5889; lr:0.000250
epoch:40; metric:emoval; train:0.7742; eval:0.5532; lr:0.000250
epoch:41; metric:emoval; train:0.7681; eval:0.5817; lr:0.000250
epoch:42; metric:emoval; train:0.7617; eval:0.5733; lr:0.000250
epoch:43; metric:emoval; train:0.7401; eval:0.5723; lr:0.000250
epoch:44; metric:emoval; train:0.7741; eval:0.6168; lr:0.000250
epoch:45; metric:emoval; train:0.7761; eval:0.5534; lr:0.000250
epoch:46; metric:emoval; train:0.7826; eval:0.5618; lr:0.000250
epoch:47; metric:emoval; train:0.7710; eval:0.5203; lr:0.000250
epoch:48; metric:emoval; train:0.7747; eval:0.5593; lr:0.000250
epoch:49; metric:emoval; train:0.7640; eval:0.5769; lr:0.000250
epoch:50; metric:emoval; train:0.7841; eval:0.5860; lr:0.000250
epoch:51; metric:emoval; train:0.7685; eval:0.5375; lr:0.000250
epoch:52; metric:emoval; train:0.7849; eval:0.5423; lr:0.000250
epoch:53; metric:emoval; train:0.7871; eval:0.5674; lr:0.000250
epoch:54; metric:emoval; train:0.7753; eval:0.5713; lr:0.000250
epoch:55; metric:emoval; train:0.8034; eval:0.5627; lr:0.000125
epoch:56; metric:emoval; train:0.8048; eval:0.5966; lr:0.000125
epoch:57; metric:emoval; train:0.8203; eval:0.5832; lr:0.000125
epoch:58; metric:emoval; train:0.8177; eval:0.5926; lr:0.000125
epoch:59; metric:emoval; train:0.8136; eval:0.5833; lr:0.000125
epoch:60; metric:emoval; train:0.8217; eval:0.5884; lr:0.000125
epoch:61; metric:emoval; train:0.8190; eval:0.5615; lr:0.000125
epoch:62; metric:emoval; train:0.8234; eval:0.5900; lr:0.000125
epoch:63; metric:emoval; train:0.8332; eval:0.5916; lr:0.000125
epoch:64; metric:emoval; train:0.8197; eval:0.5574; lr:0.000125
epoch:65; metric:emoval; train:0.8311; eval:0.5924; lr:0.000125
epoch:66; metric:emoval; train:0.8319; eval:0.5892; lr:0.000063
epoch:67; metric:emoval; train:0.8483; eval:0.5930; lr:0.000063
epoch:68; metric:emoval; train:0.8423; eval:0.6021; lr:0.000063
epoch:69; metric:emoval; train:0.8441; eval:0.5556; lr:0.000063
epoch:70; metric:emoval; train:0.8467; eval:0.5932; lr:0.000063
epoch:71; metric:emoval; train:0.8512; eval:0.5837; lr:0.000063
epoch:72; metric:emoval; train:0.8539; eval:0.5920; lr:0.000063
epoch:73; metric:emoval; train:0.8477; eval:0.5804; lr:0.000063
epoch:74; metric:emoval; train:0.8499; eval:0.5812; lr:0.000063
Early stopping at epoch 74, best epoch: 44
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 43, duration: 207.35364508628845 >>>>>
====== Prediction and Saving =======
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v6/outputs/results-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v6+utt+None_f1:0.7687_acc:0.7711_val:0.5812_1771054791.730564.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v6/outputs/results-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v6+utt+None_f1:0.8311_acc:0.8297_val:0.6301_1771054791.730564.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v6/outputs/results-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v6+utt+None_f1:0.7643_acc:0.7694_val:0.5977_1771054791.730564.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v6/outputs/results-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v6+utt+None_f1:0.8861_acc:0.8909_val:80.2571_1771054791.730564.npz
