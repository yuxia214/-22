====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, consistency_emo_weight=0.1, consistency_val_weight=0.06, contrastive_temperature=0.07, contrastive_weight=0.1, corruption_max_rate=0.55, corruption_warmup_epochs=25, cross_kl_weight=0.01, dataset='MER2023', debug=False, double_mask_ratio=0.45, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, emo_loss_weight=1.0, epochs=100, feat_scale=1, feat_type='utt', feature_noise_prob=0.35, feature_noise_std=0.02, feature_noise_warmup=5, focal_gamma=2.0, fusion_residual_scale=0.4, fusion_temperature=1.0, gate_alpha=0.55, gpu=0, grad_clip=1.0, hidden_dim=128, huber_beta=0.8, hyper_path=None, impute_loss_weight=0.12, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, latent_noise_std=0.03, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_agreement_weight=0.008, modality_dropout=0.18, modality_dropout_warmup=15, model='attention_robust_v9', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, quality_weight=0.7, recon_weight=0.1, reg_loss_type='smoothl1', reliability_temperature=0.9, save_iters=100000000.0, save_root='/root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_high_corrupt_qw0.70_iw0.12_ce0.10_cv0.06_cr0.55_dm0.45_ln0.03_20260214_195842-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_gated_uncertainty=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, use_valence_prior=True, val_loss_weight=1.4, valence_center_reg_weight=0.005, valence_consistency_weight=0.1, video_feature='clip-vit-large-patch14-UTT', weight_consistency_weight=0.02)
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 41%|████      | 1371/3373 [00:00<00:00, 13669.34it/s] 81%|████████  | 2738/3373 [00:00<00:00, 13072.48it/s]100%|██████████| 3373/3373 [00:00<00:00, 13324.96it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 30%|██▉       | 997/3373 [00:00<00:00, 9942.02it/s] 59%|█████▉    | 1992/3373 [00:00<00:00, 8569.60it/s] 85%|████████▍ | 2861/3373 [00:00<00:00, 7851.45it/s]100%|██████████| 3373/3373 [00:00<00:00, 7816.87it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 47%|████▋     | 1590/3373 [00:00<00:00, 15898.16it/s] 98%|█████████▊| 3310/3373 [00:00<00:00, 16659.85it/s]100%|██████████| 3373/3373 [00:00<00:00, 16724.02it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 10252.22it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 12387.60it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 17873.83it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 16049.38it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 12432.84it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 15765.04it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 13691.85it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14035.09it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 15440.79it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2434; eval:-0.0612; lr:0.000500
epoch:2; metric:emoval; train:-0.0248; eval:-0.0074; lr:0.000500
epoch:3; metric:emoval; train:0.1586; eval:0.0899; lr:0.000500
epoch:4; metric:emoval; train:0.2811; eval:0.2395; lr:0.000500
epoch:5; metric:emoval; train:0.3912; eval:0.4228; lr:0.000500
epoch:6; metric:emoval; train:0.4956; eval:0.4891; lr:0.000500
epoch:7; metric:emoval; train:0.5150; eval:0.5238; lr:0.000500
epoch:8; metric:emoval; train:0.5535; eval:0.4740; lr:0.000500
epoch:9; metric:emoval; train:0.5667; eval:0.5577; lr:0.000500
epoch:10; metric:emoval; train:0.5812; eval:0.4735; lr:0.000500
epoch:11; metric:emoval; train:0.5624; eval:0.5211; lr:0.000500
epoch:12; metric:emoval; train:0.5782; eval:0.5341; lr:0.000500
epoch:13; metric:emoval; train:0.5685; eval:0.5035; lr:0.000500
epoch:14; metric:emoval; train:0.5638; eval:0.5268; lr:0.000500
epoch:15; metric:emoval; train:0.5575; eval:0.5660; lr:0.000500
epoch:16; metric:emoval; train:0.5788; eval:0.4311; lr:0.000500
epoch:17; metric:emoval; train:0.5461; eval:0.4862; lr:0.000500
epoch:18; metric:emoval; train:0.5557; eval:0.4571; lr:0.000500
epoch:19; metric:emoval; train:0.5387; eval:0.5588; lr:0.000500
epoch:20; metric:emoval; train:0.5545; eval:0.5370; lr:0.000500
epoch:21; metric:emoval; train:0.5233; eval:0.4664; lr:0.000500
epoch:22; metric:emoval; train:0.5210; eval:0.5201; lr:0.000500
epoch:23; metric:emoval; train:0.5000; eval:0.5217; lr:0.000500
epoch:24; metric:emoval; train:0.5134; eval:0.5240; lr:0.000500
epoch:25; metric:emoval; train:0.4690; eval:0.5477; lr:0.000500
epoch:26; metric:emoval; train:0.4825; eval:0.5217; lr:0.000250
epoch:27; metric:emoval; train:0.5445; eval:0.5536; lr:0.000250
epoch:28; metric:emoval; train:0.5503; eval:0.5657; lr:0.000250
epoch:29; metric:emoval; train:0.5177; eval:0.5781; lr:0.000250
epoch:30; metric:emoval; train:0.5314; eval:0.5091; lr:0.000250
epoch:31; metric:emoval; train:0.5280; eval:0.5172; lr:0.000250
epoch:32; metric:emoval; train:0.5557; eval:0.5670; lr:0.000250
epoch:33; metric:emoval; train:0.5470; eval:0.5402; lr:0.000250
epoch:34; metric:emoval; train:0.5249; eval:0.5209; lr:0.000250
epoch:35; metric:emoval; train:0.5405; eval:0.5424; lr:0.000250
epoch:36; metric:emoval; train:0.5412; eval:0.5223; lr:0.000250
epoch:37; metric:emoval; train:0.5591; eval:0.5618; lr:0.000250
epoch:38; metric:emoval; train:0.5470; eval:0.4999; lr:0.000250
epoch:39; metric:emoval; train:0.5804; eval:0.5527; lr:0.000250
epoch:40; metric:emoval; train:0.5602; eval:0.5298; lr:0.000125
epoch:41; metric:emoval; train:0.5790; eval:0.5555; lr:0.000125
epoch:42; metric:emoval; train:0.6040; eval:0.5591; lr:0.000125
epoch:43; metric:emoval; train:0.5998; eval:0.5603; lr:0.000125
epoch:44; metric:emoval; train:0.5905; eval:0.5746; lr:0.000125
epoch:45; metric:emoval; train:0.6162; eval:0.5607; lr:0.000125
epoch:46; metric:emoval; train:0.6138; eval:0.5610; lr:0.000125
epoch:47; metric:emoval; train:0.6055; eval:0.5889; lr:0.000125
epoch:48; metric:emoval; train:0.5964; eval:0.5751; lr:0.000125
epoch:49; metric:emoval; train:0.5983; eval:0.5576; lr:0.000125
epoch:50; metric:emoval; train:0.6230; eval:0.5820; lr:0.000125
epoch:51; metric:emoval; train:0.6155; eval:0.5339; lr:0.000125
epoch:52; metric:emoval; train:0.5685; eval:0.5498; lr:0.000125
epoch:53; metric:emoval; train:0.6035; eval:0.5532; lr:0.000125
epoch:54; metric:emoval; train:0.5971; eval:0.5323; lr:0.000125
epoch:55; metric:emoval; train:0.6239; eval:0.5303; lr:0.000125
epoch:56; metric:emoval; train:0.6336; eval:0.4966; lr:0.000125
epoch:57; metric:emoval; train:0.6207; eval:0.5268; lr:0.000125
epoch:58; metric:emoval; train:0.6217; eval:0.5405; lr:0.000063
epoch:59; metric:emoval; train:0.6438; eval:0.5645; lr:0.000063
epoch:60; metric:emoval; train:0.6449; eval:0.5584; lr:0.000063
epoch:61; metric:emoval; train:0.6509; eval:0.5570; lr:0.000063
epoch:62; metric:emoval; train:0.6491; eval:0.5634; lr:0.000063
epoch:63; metric:emoval; train:0.6487; eval:0.5384; lr:0.000063
epoch:64; metric:emoval; train:0.6439; eval:0.5564; lr:0.000063
epoch:65; metric:emoval; train:0.6485; eval:0.5247; lr:0.000063
epoch:66; metric:emoval; train:0.6463; eval:0.5351; lr:0.000063
epoch:67; metric:emoval; train:0.6599; eval:0.5388; lr:0.000063
epoch:68; metric:emoval; train:0.6569; eval:0.5248; lr:0.000063
epoch:69; metric:emoval; train:0.6277; eval:0.5437; lr:0.000031
epoch:70; metric:emoval; train:0.6729; eval:0.5491; lr:0.000031
epoch:71; metric:emoval; train:0.6635; eval:0.5650; lr:0.000031
epoch:72; metric:emoval; train:0.6448; eval:0.5372; lr:0.000031
epoch:73; metric:emoval; train:0.6424; eval:0.5602; lr:0.000031
epoch:74; metric:emoval; train:0.6656; eval:0.5408; lr:0.000031
epoch:75; metric:emoval; train:0.6666; eval:0.5389; lr:0.000031
epoch:76; metric:emoval; train:0.6431; eval:0.5442; lr:0.000031
epoch:77; metric:emoval; train:0.6591; eval:0.5314; lr:0.000031
Early stopping at epoch 77, best epoch: 47
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 46, duration: 320.2030818462372 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3058; eval:-0.0699; lr:0.000500
epoch:2; metric:emoval; train:0.0284; eval:0.1519; lr:0.000500
epoch:3; metric:emoval; train:0.1093; eval:0.2255; lr:0.000500
epoch:4; metric:emoval; train:0.2277; eval:0.3543; lr:0.000500
epoch:5; metric:emoval; train:0.3636; eval:0.4382; lr:0.000500
epoch:6; metric:emoval; train:0.3757; eval:0.5098; lr:0.000500
epoch:7; metric:emoval; train:0.4289; eval:0.5148; lr:0.000500
epoch:8; metric:emoval; train:0.4661; eval:0.5487; lr:0.000500
epoch:9; metric:emoval; train:0.4895; eval:0.5074; lr:0.000500
epoch:10; metric:emoval; train:0.5073; eval:0.4672; lr:0.000500
epoch:11; metric:emoval; train:0.5054; eval:0.5315; lr:0.000500
epoch:12; metric:emoval; train:0.5280; eval:0.5568; lr:0.000500
epoch:13; metric:emoval; train:0.5359; eval:0.5549; lr:0.000500
epoch:14; metric:emoval; train:0.5273; eval:0.5613; lr:0.000500
epoch:15; metric:emoval; train:0.5369; eval:0.5901; lr:0.000500
epoch:16; metric:emoval; train:0.5238; eval:0.5234; lr:0.000500
epoch:17; metric:emoval; train:0.5259; eval:0.5693; lr:0.000500
epoch:18; metric:emoval; train:0.5228; eval:0.5631; lr:0.000500
epoch:19; metric:emoval; train:0.5230; eval:0.4998; lr:0.000500
epoch:20; metric:emoval; train:0.4789; eval:0.5504; lr:0.000500
epoch:21; metric:emoval; train:0.5042; eval:0.5828; lr:0.000500
epoch:22; metric:emoval; train:0.4752; eval:0.5834; lr:0.000500
epoch:23; metric:emoval; train:0.4905; eval:0.5757; lr:0.000500
epoch:24; metric:emoval; train:0.4964; eval:0.5602; lr:0.000500
epoch:25; metric:emoval; train:0.4819; eval:0.5724; lr:0.000500
epoch:26; metric:emoval; train:0.4860; eval:0.5640; lr:0.000250
epoch:27; metric:emoval; train:0.4974; eval:0.5877; lr:0.000250
epoch:28; metric:emoval; train:0.5308; eval:0.6070; lr:0.000250
epoch:29; metric:emoval; train:0.5478; eval:0.5972; lr:0.000250
epoch:30; metric:emoval; train:0.5243; eval:0.6025; lr:0.000250
epoch:31; metric:emoval; train:0.5372; eval:0.5785; lr:0.000250
epoch:32; metric:emoval; train:0.5503; eval:0.5685; lr:0.000250
epoch:33; metric:emoval; train:0.5103; eval:0.5984; lr:0.000250
epoch:34; metric:emoval; train:0.5263; eval:0.5668; lr:0.000250
epoch:35; metric:emoval; train:0.5400; eval:0.5881; lr:0.000250
epoch:36; metric:emoval; train:0.5201; eval:0.5718; lr:0.000250
epoch:37; metric:emoval; train:0.5301; eval:0.5764; lr:0.000250
epoch:38; metric:emoval; train:0.5200; eval:0.5586; lr:0.000250
epoch:39; metric:emoval; train:0.5545; eval:0.5953; lr:0.000125
epoch:40; metric:emoval; train:0.5447; eval:0.5946; lr:0.000125
epoch:41; metric:emoval; train:0.5883; eval:0.6120; lr:0.000125
epoch:42; metric:emoval; train:0.5905; eval:0.5937; lr:0.000125
epoch:43; metric:emoval; train:0.5613; eval:0.5878; lr:0.000125
epoch:44; metric:emoval; train:0.6024; eval:0.5894; lr:0.000125
epoch:45; metric:emoval; train:0.5847; eval:0.5823; lr:0.000125
epoch:46; metric:emoval; train:0.5954; eval:0.5898; lr:0.000125
epoch:47; metric:emoval; train:0.5925; eval:0.5774; lr:0.000125
epoch:48; metric:emoval; train:0.6082; eval:0.5993; lr:0.000125
epoch:49; metric:emoval; train:0.6222; eval:0.5768; lr:0.000125
epoch:50; metric:emoval; train:0.6113; eval:0.5454; lr:0.000125
epoch:51; metric:emoval; train:0.5972; eval:0.5847; lr:0.000125
epoch:52; metric:emoval; train:0.5907; eval:0.5754; lr:0.000063
epoch:53; metric:emoval; train:0.6248; eval:0.5843; lr:0.000063
epoch:54; metric:emoval; train:0.6265; eval:0.5854; lr:0.000063
epoch:55; metric:emoval; train:0.6359; eval:0.5765; lr:0.000063
epoch:56; metric:emoval; train:0.6386; eval:0.5616; lr:0.000063
epoch:57; metric:emoval; train:0.6534; eval:0.5928; lr:0.000063
epoch:58; metric:emoval; train:0.6310; eval:0.5590; lr:0.000063
epoch:59; metric:emoval; train:0.6403; eval:0.5985; lr:0.000063
epoch:60; metric:emoval; train:0.6560; eval:0.5822; lr:0.000063
epoch:61; metric:emoval; train:0.6500; eval:0.5586; lr:0.000063
epoch:62; metric:emoval; train:0.6298; eval:0.5684; lr:0.000063
epoch:63; metric:emoval; train:0.6356; eval:0.5704; lr:0.000031
epoch:64; metric:emoval; train:0.6490; eval:0.5818; lr:0.000031
epoch:65; metric:emoval; train:0.6351; eval:0.5830; lr:0.000031
epoch:66; metric:emoval; train:0.6389; eval:0.5803; lr:0.000031
epoch:67; metric:emoval; train:0.6529; eval:0.5808; lr:0.000031
epoch:68; metric:emoval; train:0.6581; eval:0.5816; lr:0.000031
epoch:69; metric:emoval; train:0.6662; eval:0.5858; lr:0.000031
epoch:70; metric:emoval; train:0.6639; eval:0.5791; lr:0.000031
epoch:71; metric:emoval; train:0.6772; eval:0.5687; lr:0.000031
Early stopping at epoch 71, best epoch: 41
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 40, duration: 302.11988139152527 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3560; eval:-0.0511; lr:0.000500
epoch:2; metric:emoval; train:-0.0046; eval:0.0232; lr:0.000500
epoch:3; metric:emoval; train:0.1927; eval:0.3319; lr:0.000500
epoch:4; metric:emoval; train:0.3248; eval:0.2604; lr:0.000500
epoch:5; metric:emoval; train:0.3757; eval:0.3904; lr:0.000500
epoch:6; metric:emoval; train:0.4927; eval:0.4991; lr:0.000500
epoch:7; metric:emoval; train:0.5122; eval:0.5500; lr:0.000500
epoch:8; metric:emoval; train:0.5469; eval:0.5494; lr:0.000500
epoch:9; metric:emoval; train:0.5472; eval:0.4730; lr:0.000500
epoch:10; metric:emoval; train:0.5566; eval:0.5404; lr:0.000500
epoch:11; metric:emoval; train:0.5248; eval:0.4458; lr:0.000500
epoch:12; metric:emoval; train:0.5959; eval:0.4980; lr:0.000500
epoch:13; metric:emoval; train:0.5691; eval:0.5052; lr:0.000500
epoch:14; metric:emoval; train:0.5623; eval:0.5193; lr:0.000500
epoch:15; metric:emoval; train:0.5663; eval:0.5365; lr:0.000500
epoch:16; metric:emoval; train:0.5738; eval:0.5291; lr:0.000500
epoch:17; metric:emoval; train:0.5700; eval:0.5038; lr:0.000500
epoch:18; metric:emoval; train:0.5530; eval:0.5461; lr:0.000250
epoch:19; metric:emoval; train:0.5837; eval:0.5751; lr:0.000250
epoch:20; metric:emoval; train:0.5899; eval:0.5661; lr:0.000250
epoch:21; metric:emoval; train:0.5789; eval:0.5569; lr:0.000250
epoch:22; metric:emoval; train:0.5708; eval:0.6059; lr:0.000250
epoch:23; metric:emoval; train:0.6061; eval:0.5749; lr:0.000250
epoch:24; metric:emoval; train:0.5624; eval:0.5252; lr:0.000250
epoch:25; metric:emoval; train:0.5216; eval:0.5576; lr:0.000250
epoch:26; metric:emoval; train:0.5493; eval:0.5107; lr:0.000250
epoch:27; metric:emoval; train:0.5605; eval:0.5630; lr:0.000250
epoch:28; metric:emoval; train:0.5215; eval:0.5550; lr:0.000250
epoch:29; metric:emoval; train:0.5322; eval:0.5692; lr:0.000250
epoch:30; metric:emoval; train:0.5167; eval:0.5402; lr:0.000250
epoch:31; metric:emoval; train:0.5363; eval:0.5748; lr:0.000250
epoch:32; metric:emoval; train:0.5253; eval:0.5850; lr:0.000250
epoch:33; metric:emoval; train:0.5244; eval:0.5026; lr:0.000125
epoch:34; metric:emoval; train:0.5602; eval:0.5711; lr:0.000125
epoch:35; metric:emoval; train:0.5420; eval:0.5784; lr:0.000125
epoch:36; metric:emoval; train:0.5804; eval:0.5647; lr:0.000125
epoch:37; metric:emoval; train:0.5726; eval:0.5456; lr:0.000125
epoch:38; metric:emoval; train:0.5815; eval:0.5776; lr:0.000125
epoch:39; metric:emoval; train:0.6030; eval:0.5566; lr:0.000125
epoch:40; metric:emoval; train:0.5677; eval:0.5551; lr:0.000125
epoch:41; metric:emoval; train:0.6063; eval:0.5719; lr:0.000125
epoch:42; metric:emoval; train:0.5830; eval:0.5593; lr:0.000125
epoch:43; metric:emoval; train:0.5681; eval:0.5621; lr:0.000125
epoch:44; metric:emoval; train:0.5919; eval:0.5864; lr:0.000063
epoch:45; metric:emoval; train:0.6159; eval:0.5638; lr:0.000063
epoch:46; metric:emoval; train:0.6093; eval:0.5790; lr:0.000063
epoch:47; metric:emoval; train:0.6334; eval:0.5758; lr:0.000063
epoch:48; metric:emoval; train:0.6159; eval:0.5672; lr:0.000063
epoch:49; metric:emoval; train:0.6138; eval:0.5604; lr:0.000063
epoch:50; metric:emoval; train:0.6167; eval:0.5449; lr:0.000063
epoch:51; metric:emoval; train:0.6263; eval:0.5718; lr:0.000063
epoch:52; metric:emoval; train:0.6294; eval:0.5555; lr:0.000063
Early stopping at epoch 52, best epoch: 22
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 21, duration: 222.6645724773407 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2282; eval:-0.0912; lr:0.000500
epoch:2; metric:emoval; train:-0.0152; eval:0.0168; lr:0.000500
epoch:3; metric:emoval; train:0.1252; eval:0.0007; lr:0.000500
epoch:4; metric:emoval; train:0.2109; eval:0.0374; lr:0.000500
epoch:5; metric:emoval; train:0.3038; eval:0.3189; lr:0.000500
epoch:6; metric:emoval; train:0.4250; eval:0.4122; lr:0.000500
epoch:7; metric:emoval; train:0.5132; eval:0.4085; lr:0.000500
epoch:8; metric:emoval; train:0.5325; eval:0.4609; lr:0.000500
epoch:9; metric:emoval; train:0.5614; eval:0.5156; lr:0.000500
epoch:10; metric:emoval; train:0.5640; eval:0.4895; lr:0.000500
epoch:11; metric:emoval; train:0.5513; eval:0.5114; lr:0.000500
epoch:12; metric:emoval; train:0.5456; eval:0.4817; lr:0.000500
epoch:13; metric:emoval; train:0.5654; eval:0.5069; lr:0.000500
epoch:14; metric:emoval; train:0.5745; eval:0.5201; lr:0.000500
epoch:15; metric:emoval; train:0.5631; eval:0.5160; lr:0.000500
epoch:16; metric:emoval; train:0.5572; eval:0.3775; lr:0.000500
epoch:17; metric:emoval; train:0.5166; eval:0.5491; lr:0.000500
epoch:18; metric:emoval; train:0.5369; eval:0.5551; lr:0.000500
epoch:19; metric:emoval; train:0.5128; eval:0.4758; lr:0.000500
epoch:20; metric:emoval; train:0.5409; eval:0.4941; lr:0.000500
epoch:21; metric:emoval; train:0.5398; eval:0.4788; lr:0.000500
epoch:22; metric:emoval; train:0.5181; eval:0.5170; lr:0.000500
epoch:23; metric:emoval; train:0.4861; eval:0.5521; lr:0.000500
epoch:24; metric:emoval; train:0.4779; eval:0.5986; lr:0.000500
epoch:25; metric:emoval; train:0.4632; eval:0.5622; lr:0.000500
epoch:26; metric:emoval; train:0.4751; eval:0.5621; lr:0.000500
epoch:27; metric:emoval; train:0.4491; eval:0.5252; lr:0.000500
epoch:28; metric:emoval; train:0.4841; eval:0.5475; lr:0.000500
epoch:29; metric:emoval; train:0.4720; eval:0.4602; lr:0.000500
epoch:30; metric:emoval; train:0.4963; eval:0.5735; lr:0.000500
epoch:31; metric:emoval; train:0.4678; eval:0.5427; lr:0.000500
epoch:32; metric:emoval; train:0.4690; eval:0.5243; lr:0.000500
epoch:33; metric:emoval; train:0.4671; eval:0.5344; lr:0.000500
epoch:34; metric:emoval; train:0.4785; eval:0.5830; lr:0.000500
epoch:35; metric:emoval; train:0.4606; eval:0.5130; lr:0.000250
epoch:36; metric:emoval; train:0.4947; eval:0.5780; lr:0.000250
epoch:37; metric:emoval; train:0.5428; eval:0.5673; lr:0.000250
epoch:38; metric:emoval; train:0.5396; eval:0.5631; lr:0.000250
epoch:39; metric:emoval; train:0.5236; eval:0.4937; lr:0.000250
epoch:40; metric:emoval; train:0.5501; eval:0.5639; lr:0.000250
epoch:41; metric:emoval; train:0.5601; eval:0.5581; lr:0.000250
epoch:42; metric:emoval; train:0.5454; eval:0.5649; lr:0.000250
epoch:43; metric:emoval; train:0.5232; eval:0.6008; lr:0.000250
epoch:44; metric:emoval; train:0.5421; eval:0.5841; lr:0.000250
epoch:45; metric:emoval; train:0.5376; eval:0.5492; lr:0.000250
epoch:46; metric:emoval; train:0.5570; eval:0.5782; lr:0.000250
epoch:47; metric:emoval; train:0.5672; eval:0.5820; lr:0.000250
epoch:48; metric:emoval; train:0.5483; eval:0.5731; lr:0.000250
epoch:49; metric:emoval; train:0.5613; eval:0.5579; lr:0.000250
epoch:50; metric:emoval; train:0.5570; eval:0.5100; lr:0.000250
epoch:51; metric:emoval; train:0.5664; eval:0.5514; lr:0.000250
epoch:52; metric:emoval; train:0.5678; eval:0.5640; lr:0.000250
epoch:53; metric:emoval; train:0.5748; eval:0.5357; lr:0.000250
epoch:54; metric:emoval; train:0.5633; eval:0.5500; lr:0.000125
epoch:55; metric:emoval; train:0.5933; eval:0.5595; lr:0.000125
epoch:56; metric:emoval; train:0.5921; eval:0.6005; lr:0.000125
epoch:57; metric:emoval; train:0.6172; eval:0.5948; lr:0.000125
epoch:58; metric:emoval; train:0.6312; eval:0.5744; lr:0.000125
epoch:59; metric:emoval; train:0.6119; eval:0.5715; lr:0.000125
epoch:60; metric:emoval; train:0.6271; eval:0.5700; lr:0.000125
epoch:61; metric:emoval; train:0.6122; eval:0.5683; lr:0.000125
epoch:62; metric:emoval; train:0.6252; eval:0.5462; lr:0.000125
epoch:63; metric:emoval; train:0.6139; eval:0.5680; lr:0.000125
epoch:64; metric:emoval; train:0.6206; eval:0.5762; lr:0.000125
epoch:65; metric:emoval; train:0.6283; eval:0.5992; lr:0.000063
epoch:66; metric:emoval; train:0.6539; eval:0.5859; lr:0.000063
epoch:67; metric:emoval; train:0.6339; eval:0.6062; lr:0.000063
epoch:68; metric:emoval; train:0.6202; eval:0.5650; lr:0.000063
epoch:69; metric:emoval; train:0.6559; eval:0.5754; lr:0.000063
epoch:70; metric:emoval; train:0.6642; eval:0.5947; lr:0.000063
epoch:71; metric:emoval; train:0.6254; eval:0.5779; lr:0.000063
epoch:72; metric:emoval; train:0.6514; eval:0.5667; lr:0.000063
epoch:73; metric:emoval; train:0.6453; eval:0.5435; lr:0.000063
epoch:74; metric:emoval; train:0.6314; eval:0.5587; lr:0.000063
epoch:75; metric:emoval; train:0.6214; eval:0.5872; lr:0.000063
epoch:76; metric:emoval; train:0.6474; eval:0.5791; lr:0.000063
epoch:77; metric:emoval; train:0.6289; eval:0.5942; lr:0.000063
epoch:78; metric:emoval; train:0.6414; eval:0.5715; lr:0.000031
epoch:79; metric:emoval; train:0.6677; eval:0.5711; lr:0.000031
epoch:80; metric:emoval; train:0.6697; eval:0.5812; lr:0.000031
epoch:81; metric:emoval; train:0.6576; eval:0.5822; lr:0.000031
epoch:82; metric:emoval; train:0.6762; eval:0.5751; lr:0.000031
epoch:83; metric:emoval; train:0.6574; eval:0.5633; lr:0.000031
epoch:84; metric:emoval; train:0.6491; eval:0.5849; lr:0.000031
epoch:85; metric:emoval; train:0.6736; eval:0.5773; lr:0.000031
epoch:86; metric:emoval; train:0.6634; eval:0.5639; lr:0.000031
epoch:87; metric:emoval; train:0.6809; eval:0.5665; lr:0.000031
epoch:88; metric:emoval; train:0.6834; eval:0.5818; lr:0.000031
epoch:89; metric:emoval; train:0.6833; eval:0.5784; lr:0.000016
epoch:90; metric:emoval; train:0.6968; eval:0.5757; lr:0.000016
epoch:91; metric:emoval; train:0.6733; eval:0.5763; lr:0.000016
epoch:92; metric:emoval; train:0.6837; eval:0.5793; lr:0.000016
epoch:93; metric:emoval; train:0.6788; eval:0.5707; lr:0.000016
epoch:94; metric:emoval; train:0.6800; eval:0.5741; lr:0.000016
epoch:95; metric:emoval; train:0.6686; eval:0.5839; lr:0.000016
epoch:96; metric:emoval; train:0.6762; eval:0.5740; lr:0.000016
epoch:97; metric:emoval; train:0.6844; eval:0.5541; lr:0.000016
Early stopping at epoch 97, best epoch: 67
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 66, duration: 450.08717942237854 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1814; eval:0.1245; lr:0.000500
epoch:2; metric:emoval; train:0.0207; eval:0.0953; lr:0.000500
epoch:3; metric:emoval; train:0.1243; eval:0.2041; lr:0.000500
epoch:4; metric:emoval; train:0.1197; eval:0.1879; lr:0.000500
epoch:5; metric:emoval; train:0.1262; eval:0.2888; lr:0.000500
epoch:6; metric:emoval; train:0.1795; eval:0.4230; lr:0.000500
epoch:7; metric:emoval; train:0.2687; eval:0.4743; lr:0.000500
epoch:8; metric:emoval; train:0.3433; eval:0.4522; lr:0.000500
epoch:9; metric:emoval; train:0.3995; eval:0.5561; lr:0.000500
epoch:10; metric:emoval; train:0.4201; eval:0.5226; lr:0.000500
epoch:11; metric:emoval; train:0.4361; eval:0.5770; lr:0.000500
epoch:12; metric:emoval; train:0.4894; eval:0.5654; lr:0.000500
epoch:13; metric:emoval; train:0.4860; eval:0.6068; lr:0.000500
epoch:14; metric:emoval; train:0.4946; eval:0.5616; lr:0.000500
epoch:15; metric:emoval; train:0.4940; eval:0.5907; lr:0.000500
epoch:16; metric:emoval; train:0.4689; eval:0.5348; lr:0.000500
epoch:17; metric:emoval; train:0.4949; eval:0.5695; lr:0.000500
epoch:18; metric:emoval; train:0.4822; eval:0.6046; lr:0.000500
epoch:19; metric:emoval; train:0.4844; eval:0.5721; lr:0.000500
epoch:20; metric:emoval; train:0.4652; eval:0.5954; lr:0.000500
epoch:21; metric:emoval; train:0.4687; eval:0.4601; lr:0.000500
epoch:22; metric:emoval; train:0.3158; eval:0.5784; lr:0.000500
epoch:23; metric:emoval; train:0.3608; eval:0.6044; lr:0.000500
epoch:24; metric:emoval; train:0.4212; eval:0.5714; lr:0.000250
epoch:25; metric:emoval; train:0.4692; eval:0.5701; lr:0.000250
epoch:26; metric:emoval; train:0.4539; eval:0.6340; lr:0.000250
epoch:27; metric:emoval; train:0.4561; eval:0.6009; lr:0.000250
epoch:28; metric:emoval; train:0.4781; eval:0.5613; lr:0.000250
epoch:29; metric:emoval; train:0.4552; eval:0.6055; lr:0.000250
epoch:30; metric:emoval; train:0.4887; eval:0.6002; lr:0.000250
epoch:31; metric:emoval; train:0.4866; eval:0.6154; lr:0.000250
epoch:32; metric:emoval; train:0.4926; eval:0.5539; lr:0.000250
epoch:33; metric:emoval; train:0.4990; eval:0.6019; lr:0.000250
epoch:34; metric:emoval; train:0.5074; eval:0.5656; lr:0.000250
epoch:35; metric:emoval; train:0.4905; eval:0.6036; lr:0.000250
epoch:36; metric:emoval; train:0.5191; eval:0.6209; lr:0.000250
epoch:37; metric:emoval; train:0.4960; eval:0.5742; lr:0.000125
epoch:38; metric:emoval; train:0.5469; eval:0.6184; lr:0.000125
epoch:39; metric:emoval; train:0.5779; eval:0.6029; lr:0.000125
epoch:40; metric:emoval; train:0.5499; eval:0.5980; lr:0.000125
epoch:41; metric:emoval; train:0.5628; eval:0.6110; lr:0.000125
epoch:42; metric:emoval; train:0.5668; eval:0.6033; lr:0.000125
epoch:43; metric:emoval; train:0.5661; eval:0.5903; lr:0.000125
epoch:44; metric:emoval; train:0.5613; eval:0.5718; lr:0.000125
epoch:45; metric:emoval; train:0.5545; eval:0.6044; lr:0.000125
epoch:46; metric:emoval; train:0.5545; eval:0.6173; lr:0.000125
epoch:47; metric:emoval; train:0.5823; eval:0.6233; lr:0.000125
epoch:48; metric:emoval; train:0.5568; eval:0.5816; lr:0.000063
epoch:49; metric:emoval; train:0.5919; eval:0.6298; lr:0.000063
epoch:50; metric:emoval; train:0.6109; eval:0.6351; lr:0.000063
epoch:51; metric:emoval; train:0.5840; eval:0.6150; lr:0.000063
epoch:52; metric:emoval; train:0.5831; eval:0.5986; lr:0.000063
epoch:53; metric:emoval; train:0.6116; eval:0.6257; lr:0.000063
epoch:54; metric:emoval; train:0.6214; eval:0.6123; lr:0.000063
epoch:55; metric:emoval; train:0.5956; eval:0.6162; lr:0.000063
epoch:56; metric:emoval; train:0.6052; eval:0.6147; lr:0.000063
epoch:57; metric:emoval; train:0.5882; eval:0.6251; lr:0.000063
epoch:58; metric:emoval; train:0.5905; eval:0.6100; lr:0.000063
epoch:59; metric:emoval; train:0.6420; eval:0.6121; lr:0.000063
epoch:60; metric:emoval; train:0.5885; eval:0.6026; lr:0.000063
epoch:61; metric:emoval; train:0.6131; eval:0.6158; lr:0.000031
epoch:62; metric:emoval; train:0.6170; eval:0.6175; lr:0.000031
epoch:63; metric:emoval; train:0.6192; eval:0.6210; lr:0.000031
epoch:64; metric:emoval; train:0.6303; eval:0.6153; lr:0.000031
epoch:65; metric:emoval; train:0.6219; eval:0.6044; lr:0.000031
epoch:66; metric:emoval; train:0.6036; eval:0.6250; lr:0.000031
epoch:67; metric:emoval; train:0.5984; eval:0.6125; lr:0.000031
epoch:68; metric:emoval; train:0.6012; eval:0.6164; lr:0.000031
epoch:69; metric:emoval; train:0.6269; eval:0.6125; lr:0.000031
epoch:70; metric:emoval; train:0.6378; eval:0.6264; lr:0.000031
epoch:71; metric:emoval; train:0.6289; eval:0.6100; lr:0.000031
epoch:72; metric:emoval; train:0.6272; eval:0.6013; lr:0.000016
epoch:73; metric:emoval; train:0.6375; eval:0.6094; lr:0.000016
epoch:74; metric:emoval; train:0.6398; eval:0.6177; lr:0.000016
epoch:75; metric:emoval; train:0.6369; eval:0.6328; lr:0.000016
epoch:76; metric:emoval; train:0.6640; eval:0.6226; lr:0.000016
epoch:77; metric:emoval; train:0.6480; eval:0.6206; lr:0.000016
epoch:78; metric:emoval; train:0.6330; eval:0.6116; lr:0.000016
epoch:79; metric:emoval; train:0.6396; eval:0.6132; lr:0.000016
epoch:80; metric:emoval; train:0.6553; eval:0.6115; lr:0.000016
Early stopping at epoch 80, best epoch: 50
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 49, duration: 368.2292990684509 >>>>>
====== Prediction and Saving =======
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_high_corrupt_qw0.70_iw0.12_ce0.10_cv0.06_cr0.55_dm0.45_ln0.03_20260214_195842-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.7627_acc:0.7646_val:0.6123_1771071624.5483248.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_high_corrupt_qw0.70_iw0.12_ce0.10_cv0.06_cr0.55_dm0.45_ln0.03_20260214_195842-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.8120_acc:0.8102_val:0.6482_1771071624.5483248.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_high_corrupt_qw0.70_iw0.12_ce0.10_cv0.06_cr0.55_dm0.45_ln0.03_20260214_195842-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.7410_acc:0.7451_val:0.6694_1771071624.5483248.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_high_corrupt_qw0.70_iw0.12_ce0.10_cv0.06_cr0.55_dm0.45_ln0.03_20260214_195842-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.8848_acc:0.8885_val:81.3115_1771071624.5483248.npz
